{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle\n",
    "\n",
    "from Extractor_module import SignalExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data/result_df.csv\"\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "METRIC = [\"EDA\", \"TEMP\", \"BVP\", \"ACC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for Plotting with Matplotlib\n",
    "\n",
    "In this section, we define methods to visualize data distributions using Matplotlib. These methods help in understanding the class distribution before and after applying SMOTE, as well as the distribution of datasets across training, validation, and test sets.\n",
    "\n",
    "### Plotting Class Distribution Before and After SMOTE\n",
    "\n",
    "The `plot_smote_class_distribution` function plots the class distribution of the training labels before and after applying SMOTE (Synthetic Minority Over-sampling Technique). This visualization helps in understanding how SMOTE balances the class distribution.\n",
    "\n",
    "\n",
    "### Plotting Dataset Distribution\n",
    "\n",
    "The `plot_dataset_distribution` function plots a bar chart showing the sizes of the train, validation, and test sets. This visualization helps in understanding the distribution of samples across different datasets.\n",
    "\n",
    "These methods provide visual insights into the data, which is crucial for understanding and improving the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smote_class_distribution(y_train, y_train_resampled):\n",
    "    \"\"\"\n",
    "    Plots the class distribution before and after applying SMOTE.\n",
    "\n",
    "    Parameters:\n",
    "    - y_train: Original training labels.\n",
    "    - y_train_resampled: Training labels after applying SMOTE.\n",
    "    \"\"\"    \n",
    "    # Class distribution before SMOTE\n",
    "    class_distribution_before = Counter(y_train)\n",
    "    print(len(class_distribution_before))\n",
    "    # Class distribution after SMOTE\n",
    "    class_distribution_after = Counter(y_train_resampled)\n",
    "\n",
    "    # Define labels\n",
    "    labels = ['No Stress', 'Stress']\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot before SMOTE\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(labels, class_distribution_before.values(), color='blue')\n",
    "    plt.title('Class Distribution Before SMOTE')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], labels)\n",
    "\n",
    "    # Plot after SMOTE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(labels, class_distribution_after.values(), color='green')\n",
    "    plt.title('Class Distribution After SMOTE')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], labels)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dataset_distribution(x_train, y_train, x_val, y_val, x_test_1, y_test_1, x_test_2, y_test_2):\n",
    "    dataset_names = ['Train','Validation', 'Test Subject S16', 'Test Subject S17']\n",
    "    \n",
    "    # Calculate the lengths for 'EDA' signal in each dataset\n",
    "    x_lengths = [\n",
    "        len(x_train['EDA']), \n",
    "        len(x_val['EDA']), \n",
    "        len(x_test_1['EDA']), \n",
    "        len(x_test_2['EDA'])\n",
    "    ]\n",
    "    \n",
    "    # Calculate the lengths for y datasets (labels)\n",
    "    y_lengths = [\n",
    "        len(y_train), \n",
    "        len(y_val), \n",
    "        len(y_test_1), \n",
    "        len(y_test_2)\n",
    "    ]\n",
    "    \n",
    "    # Plotting the bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    bar_width = 0.35  # Width of the bars\n",
    "    index = np.arange(len(dataset_names))  # Positions for the bars\n",
    "    \n",
    "    # Create bar plots for x and y datasets\n",
    "    plt.bar(index, x_lengths, bar_width, color='b', alpha=0.6, label='X (Feature: EDA)')\n",
    "    plt.bar(index + bar_width, y_lengths, bar_width, color='r', alpha=0.6, label='Y (Labels)')\n",
    "    \n",
    "    # Labels, title, etc.\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Dataset Distribution')\n",
    "    plt.xticks(index + bar_width / 2, dataset_names)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(file_path, chunk_size=100000, usecols=None, dtype=None):\n",
    "    \"\"\"Reads a CSV file in chunks, adds a subject_id column, and returns a list of DataFrames for each chunk.\"\"\"\n",
    "    dataframes = []\n",
    "    try:\n",
    "        # Read the CSV in chunks\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size, usecols=usecols, dtype=dtype):\n",
    "            dataframes.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading chunks from {file_path}: {e}\")\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(subjects, sensors=(\"unknown\"), chunk_size=100000, usecols=None, dtype=None):\n",
    "    \"\"\"Load data from multiple CSV files in chunks and concatenate them into a single DataFrame.\"\"\"\n",
    "    dataframes = []\n",
    "        # Create file paths for each subject and sensor\n",
    "    file_paths = [MAIN_PATH + f\"/data/WESAD/{subject}/{subject}_{sensors}_data.csv\" for subject in subjects]\n",
    "    total_files = len(file_paths)\n",
    "\n",
    "    # Use a thread pool to read files in chunks in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(read_chunk, file_path, chunk_size, usecols, dtype): file_path for file_path in file_paths}\n",
    "\n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            file_path = futures[future]\n",
    "            try:\n",
    "                # Extend dataframes with the result of read_chunk\n",
    "                dataframes.extend(future.result())\n",
    "                print(f\"Completed reading all chunks from {file_path} ({i + 1}/{total_files}).\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunks from {file_path}: {e}\")\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    all_data = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Sort by subject_id and time columns, assuming 'time' column exists in CSVs\n",
    "    return all_data.sort_values(by=[\"ID\", \"StartTime\"]).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_subjects(dictionary):\n",
    "    \"\"\"\n",
    "    Filters the subjects' data from the dictionary and removes any suffix from the subject keys, \n",
    "    retaining only the `S{number}` format. Separates test subjects S16 and S17 from the rest.\n",
    "    \"\"\"\n",
    "    test_subject_1 = None\n",
    "    test_subject_2 = None\n",
    "    sequence_data = {}\n",
    "\n",
    "    # Loop through the dictionary to filter data by subject ID\n",
    "    for key, value in dictionary.items():\n",
    "        # Extract only the subject ID (S{number}) using regex\n",
    "        match = re.match(r\"(S\\d+)\", key)\n",
    "        if match:\n",
    "            subject_id = match.group(1)  # Get only S{number}, e.g., \"S11\"\n",
    "\n",
    "            # Check for specific test subjects (S16 and S17) or add to the sequence_data\n",
    "            if subject_id == 'S16':\n",
    "                test_subject_1 = value\n",
    "            elif subject_id == 'S17':\n",
    "                test_subject_2 = value\n",
    "            else:\n",
    "                sequence_data[subject_id] = value  # Store the data with cleaned-up subject_id\n",
    "        else:\n",
    "            print(f\"Warning: Could not determine subject ID format for key '{key}'.\")\n",
    "\n",
    "    # Check if data for S16 and S17 were found, log warnings if missing\n",
    "    if test_subject_1 is None:\n",
    "        print(\"Warning: No data found for test subject 1 (ID S16)\")\n",
    "    if test_subject_2 is None:\n",
    "        print(\"Warning: No data found for test subject 2 (ID S17)\")\n",
    "\n",
    "    return sequence_data, test_subject_1, test_subject_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sequences_dict, features):\n",
    "    \"\"\"\n",
    "    Prepare the data by extracting the specified features from the sequences dictionary.\n",
    "\n",
    "    Args:\n",
    "        sequences_dict (dict): Dictionary containing sequences of sensor data, where each key is a subject, \n",
    "                             and each value is a dictionary of features (e.g., {'EDA': data, 'TEMP': data}).\n",
    "        features (list): List of features to extract from each subject's data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary where each key is a subject, and each value is a dictionary of feature arrays.\n",
    "              Example format: {'subject_key': {'EDA': array, 'TEMP': array, ...}, ...}\n",
    "    \"\"\"\n",
    "    feature_arrays = {}\n",
    "\n",
    "    # Loop through each subject in the sequences_dict dictionary\n",
    "    for subject_key, subject_data in sequences_dict.items():\n",
    "        # Initialize a dictionary to hold the features for the current subject\n",
    "        subject_features = {}\n",
    "\n",
    "        # Loop through the list of features to extract\n",
    "        for feature in features:\n",
    "            if feature in subject_data:\n",
    "                # Convert feature data into a numpy array and add it to subject_features\n",
    "                subject_features[feature] = np.array(subject_data[feature])\n",
    "            else:\n",
    "                print(f\"Warning: Feature '{feature}' not found in subject '{subject_key}'.\")\n",
    "\n",
    "        # Add the subject's feature data to the main feature_arrays dictionary\n",
    "        feature_arrays[subject_key] = subject_features\n",
    "\n",
    "    return feature_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def apply_smote(x_train, y_train):\n",
    "    \"\"\"\n",
    "    Reshape x_train to have signal types as keys and apply SMOTE to balance the dataset.\n",
    "    \n",
    "    Args:\n",
    "        x_train (dict): Dictionary where each subject has signal data (BVP, ACC, TEMP, EDA).\n",
    "                        The shape of each signal is (num_samples, sequence_length, 1).\n",
    "        y_train (dict): Dictionary of labels for each subject, where each value is a (num_samples,) array.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Resampled x_train (with signals as keys) and resampled y_train.\n",
    "    \"\"\"\n",
    "    # Initialize lists to hold combined data for SMOTE\n",
    "    combined_features = {'BVP': [], 'ACC': [], 'TEMP': [], 'EDA': []}\n",
    "    combined_labels = []\n",
    "\n",
    "    # Step 1: Organize x_train into signal-based keys and flatten data\n",
    "    for subject, data in x_train.items():\n",
    "        for signal, signal_data in data.items():\n",
    "            # Flatten signal data (num_samples, sequence_length, 1) -> (num_samples, sequence_length)\n",
    "            combined_features[signal].append(signal_data.reshape(signal_data.shape[0], -1))\n",
    "        \n",
    "        # Add labels for this subject\n",
    "        combined_labels.extend(y_train[subject].ravel())  # Flatten the labels\n",
    "\n",
    "    # Step 2: Stack features by signal type into single arrays\n",
    "    for signal in combined_features:\n",
    "        combined_features[signal] = np.vstack(combined_features[signal])  # Shape: (total_windows, signal_features)\n",
    "\n",
    "    # Stack all signals into a combined feature array\n",
    "    X_combined = np.hstack([combined_features['BVP'], combined_features['ACC'], combined_features['TEMP'], combined_features['EDA']])\n",
    "    y_combined = np.array(combined_labels)  # Shape: (total_windows,)\n",
    "\n",
    "    # Step 3: Apply SMOTE to the flattened feature set\n",
    "    smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_combined, y_combined)\n",
    "\n",
    "    # Step 4: Reshape the resampled data back into the original signal format\n",
    "    num_resampled_samples = X_resampled.shape[0]\n",
    "    bvp_size = combined_features['BVP'].shape[1]\n",
    "    acc_size = combined_features['ACC'].shape[1]\n",
    "    temp_size = combined_features['TEMP'].shape[1]\n",
    "    eda_size = combined_features['EDA'].shape[1]\n",
    "\n",
    "    # Reshape the resampled data back to its original shape\n",
    "    bvp_resampled = X_resampled[:, :bvp_size].reshape(num_resampled_samples, bvp_size, 1)\n",
    "    acc_resampled = X_resampled[:, bvp_size:bvp_size+acc_size].reshape(num_resampled_samples, acc_size, 1)\n",
    "    temp_resampled = X_resampled[:, bvp_size+acc_size:bvp_size+acc_size+temp_size].reshape(num_resampled_samples, temp_size, 1)\n",
    "    eda_resampled = X_resampled[:, bvp_size+acc_size+temp_size:].reshape(num_resampled_samples, eda_size, 1)\n",
    "\n",
    "    # Step 5: Reconstruct x_train with signals as keys\n",
    "    x_train_resampled = {\n",
    "        'BVP': bvp_resampled,\n",
    "        'ACC': acc_resampled,\n",
    "        'TEMP': temp_resampled,\n",
    "        'EDA': eda_resampled\n",
    "    }\n",
    "\n",
    "    return x_train_resampled, y_resampled.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def split_data(feature_arrays, y, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Split the data into training and validation sets for each feature, handling each feature's specific dimensions.\n",
    "    \n",
    "    Args:\n",
    "        feature_arrays (dict): Dictionary where each key is a subject ID, and each value is a dictionary of features.\n",
    "                               Example: {'subject_1': {'EDA': array, 'TEMP': array, ...}, ...}\n",
    "        y (dict): Dictionary containing labels corresponding to each subject.\n",
    "        test_size (float): Proportion of the dataset to include in the validation split.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Dictionaries containing the training and validation splits for each subject, each feature, and the labels.\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries to hold the training and validation sets\n",
    "    x_train, x_val = {}, {}\n",
    "    y_train, y_val = {}, {}\n",
    "\n",
    "    # Split for each subject individually\n",
    "    for subject, features in feature_arrays.items():\n",
    "        x_train[subject] = {}\n",
    "        x_val[subject] = {}\n",
    "\n",
    "        # Collect labels for the current subject (assuming it's scalar or needs replication)\n",
    "        y_subject = y[subject]\n",
    "\n",
    "        # Ensure y_subject is in an appropriate shape for splitting\n",
    "        if isinstance(y_subject, (np.ndarray, list)) and len(y_subject) == 1:\n",
    "            y_subject = np.array([y_subject[0]] * len(features['EDA']))  # Repeat the label for all data points\n",
    "\n",
    "        # Perform train-test split for each feature for the current subject\n",
    "        for feature_name, feature_data in features.items():\n",
    "            # Ensure feature_data is a numpy array\n",
    "            feature_data = np.array(feature_data)\n",
    "\n",
    "            # Split feature data with corresponding labels\n",
    "            x_train_feature, x_val_feature, y_train_subject, y_val_subject = train_test_split(\n",
    "                feature_data, y_subject, test_size=test_size, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Store split data in nested dictionaries for the subject\n",
    "            x_train[subject][feature_name] = x_train_feature\n",
    "            x_val[subject][feature_name] = x_val_feature\n",
    "\n",
    "        # Store split labels for the subject (they are the same across features)\n",
    "        y_train[subject] = y_train_subject\n",
    "        y_val[subject] = y_val_subject\n",
    "    \n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_from_data(data, subject_name):\n",
    "    \"\"\"\n",
    "    Extracts labels from a numpy array. Raises an error for unsupported data types.\n",
    "    \"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        print(f\"Labels count for {subject_name}: {len(data)}\")\n",
    "        return data\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type '{type(data)}' for {subject_name}. Expected numpy.ndarray.\")\n",
    "\n",
    "def extract_labels(dictionary, t_df1, t_df2, label_column='labels'):\n",
    "    \"\"\"\n",
    "    Extracts labels for the entire dataset, test subject 1, and test subject 2.\n",
    "    \"\"\"\n",
    "    # Ensure the dictionary is of type 'dict', otherwise raise an error\n",
    "    if not isinstance(dictionary, dict):\n",
    "        raise TypeError(f\"Expected 'dictionary' to be of type 'dict', but got {type(dictionary)}\")\n",
    "\n",
    "    # Extract labels for test subjects S16 and S17 using the helper function\n",
    "    try:\n",
    "        validation_labels_subject_1 = extract_labels_from_data(t_df1[label_column], \"Subject 1 (S16)\")\n",
    "        validation_labels_subject_2 = extract_labels_from_data(t_df2[label_column], \"Subject 2 (S17)\")\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Missing key '{label_column}' in test subject data: {e}\")\n",
    "\n",
    "    # Extract labels for all subjects in the dictionary\n",
    "    all_labels = {}\n",
    "    for subject_key, subject_data in dictionary.items():\n",
    "        try:\n",
    "            if label_column in subject_data:\n",
    "                subject_labels = subject_data[label_column]\n",
    "                all_labels[subject_key] = subject_labels\n",
    "            else:\n",
    "                print(f\"Warning: '{label_column}' not found for {subject_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {subject_key}: {e}\")\n",
    "\n",
    "    return all_labels, validation_labels_subject_1, validation_labels_subject_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets_to_pickle(x_train, y_train, x_val, y_val, x_test_1, y_test_1, x_test_2, y_test_2, save_directory=\"../data/results/\"):\n",
    "    \"\"\"Save the datasets to a pickle file.\n",
    "    Args:\n",
    "        x_train (ndarray): Training feature data.\n",
    "        y_train (ndarray): Training label data.\n",
    "        x_val (ndarray): Validation feature data.\n",
    "        y_val (ndarray): Validation label data.\n",
    "        x_test_1 (ndarray): Test feature data for Subject 1.\n",
    "        y_test_1 (ndarray): Test label data for Subject 1.\n",
    "        x_test_2 (ndarray): Test feature data for Subject 2.\n",
    "        y_test_2 (ndarray): Test label data for Subject 2.\n",
    "        filename (str): The name of the file to save the datasets to (should end with .pkl).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Dictionary to store all datasets\n",
    "        datasets = {\n",
    "            \"x_train\": x_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"x_val\": x_val,\n",
    "            \"y_val\": y_val,\n",
    "            \"x_test_1\": x_test_1,\n",
    "            \"y_test_1\": y_test_1,\n",
    "            \"x_test_2\": x_test_2,\n",
    "            \"y_test_2\": y_test_2\n",
    "        }\n",
    "        \n",
    "        # Save each dataset as a separate pickle file\n",
    "        for name, data in datasets.items():\n",
    "            save_path = f\"{save_directory}{name}.pkl\"\n",
    "            with open(save_path, 'wb') as file:\n",
    "                pickle.dump(data, file)\n",
    "            print(f\"{name} saved successfully to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save datasets: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dict_to_ndarray(data_dict):\n",
    "    \"\"\"\n",
    "    Converts a nested dictionary of subjects and signals to a single NumPy array.\n",
    "    \n",
    "    Args:\n",
    "        data_dict (dict): Nested dictionary where each subject contains signals, and each signal is an ndarray.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: 2D array where each row is a flattened window from each signal for each subject.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    for subject, signals in data_dict.items():\n",
    "        # Flatten each signal and concatenate along axis 1 (feature dimension)\n",
    "        flattened_signals = np.hstack([signal.reshape(signal.shape[0], -1) for signal in signals.values()])\n",
    "        combined_data.append(flattened_signals)\n",
    "    \n",
    "    # Stack all subjects' data into a single array\n",
    "    return np.vstack(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels count for Subject 1 (S16): 703\n",
      "Labels count for Subject 2 (S17): 740\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'ravel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     plot_dataset_distribution(x_train_resampled, y_train_resampled, x_val_resampled, y_val_resampled, test_dataset1, y_test_subject_1, test_dataset2, y_test_subject_2)    \n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# save_datasets_to_pickle(x_train_resampled, y_train_resampled, x_val_resampled, y_val_resampled, test_dataset1, y_test_subject_1, test_dataset2, y_test_subject_2)\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[145], line 23\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m x_val_resampled, y_val_resampled \u001b[38;5;241m=\u001b[39m apply_smote(x_val, y_val)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Plot class distribution before and after SMOTE\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mplot_smote_class_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Reshape the test data to match the expected input shape\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "Cell \u001b[1;32mIn[134], line 10\u001b[0m, in \u001b[0;36mplot_smote_class_distribution\u001b[1;34m(y_train, y_train_resampled)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mPlots the class distribution before and after applying SMOTE.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m- y_train_resampled: Training labels after applying SMOTE.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m    \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Class distribution before SMOTE\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m class_distribution_before \u001b[38;5;241m=\u001b[39m Counter(\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m())\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(class_distribution_before))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Class distribution after SMOTE\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'ravel'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    features = ['EDA', 'BVP', 'ACC', 'TEMP']\n",
    "\n",
    "    # Load the data from CSV files\n",
    "    dictionary = SignalExtractor.load_data_from_pickle(r'C:\\Master of Applied IT\\data\\processed_data')\n",
    "\n",
    "    # Filter the data for the test subjects and the rest\n",
    "    sequence_data, test_dataset1, test_dataset2 = filter_subjects(dictionary)\n",
    "\n",
    "    # Extract labels for the training and test data\n",
    "    y_sequence_data, y_test_subject_1, y_test_subject_2 = extract_labels(sequence_data, test_dataset1, test_dataset2)\n",
    "   \n",
    "    # Prepare the data for training\n",
    "    feature_data = prepare_data(sequence_data, features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = split_data(feature_data, y_sequence_data, test_size=0.3)\n",
    "\n",
    "    # Apply SMOTE to balance the training data\n",
    "    x_train_resampled, y_train_resampled = apply_smote(x_train, y_train)\n",
    "    x_val_resampled, y_val_resampled = apply_smote(x_val, y_val)\n",
    "    # Plot class distribution before and after SMOTE\n",
    "    plot_smote_class_distribution(y_train, y_train_resampled)\n",
    "    \n",
    "    # Reshape the test data to match the expected input shape\n",
    "    for feature in features:\n",
    "        test_dataset1[feature] = test_dataset1[feature].reshape(test_dataset1[feature].shape[0], test_dataset1[feature].shape[1], 1)\n",
    "        test_dataset2[feature] = test_dataset2[feature].reshape(test_dataset2[feature].shape[0], test_dataset2[feature].shape[1], 1)\n",
    "    \n",
    "    y_test_subject_1 = y_test_subject_1.reshape(-1, 1)\n",
    "    y_test_subject_2 = y_test_subject_2.reshape(-1, 1)\n",
    "    \n",
    "    print(f\"Resampled training data shapes: {x_train_resampled['BVP'].shape}, {y_train_resampled.shape}\")\n",
    "    print(f\"Resampled validation data shapes: {x_val_resampled['BVP'].shape}, {y_val_resampled.shape}\")\n",
    "\n",
    "    # x_train_restructured = SignalExtractor.restructure_x_train(x_train_resampled)\n",
    "    # x_val_restructured, y_val_restructured = SignalExtractor.restructure_x_train_and_y_val(x_val, y_val)\n",
    "\n",
    "    print(f\"Training data shapes: {x_train_resampled['BVP'].shape}, {y_train_resampled.shape}\")\n",
    "    print(f\"Validation data shapes: {x_val_resampled['BVP'].shape}, {y_val_resampled.shape}\")\n",
    "    # Reshape test data to match the expected input shape\n",
    "\n",
    "    print(f\"Test data shapes: {test_dataset1['BVP'].shape}, {y_test_subject_1.shape}\")\n",
    "\n",
    "\n",
    "    plot_dataset_distribution(x_train_resampled, y_train_resampled, x_val_resampled, y_val_resampled, test_dataset1, y_test_subject_1, test_dataset2, y_test_subject_2)    \n",
    "    # save_datasets_to_pickle(x_train_resampled, y_train_resampled, x_val_resampled, y_val_resampled, test_dataset1, y_test_subject_1, test_dataset2, y_test_subject_2)\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

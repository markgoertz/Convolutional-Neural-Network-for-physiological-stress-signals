{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data/result_df.csv\"\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "METRIC = [\"EDA\", \"TEMP\", \"BVP\", \"ACC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for Plotting with Matplotlib\n",
    "\n",
    "In this section, we define methods to visualize data distributions using Matplotlib. These methods help in understanding the class distribution before and after applying SMOTE, as well as the distribution of datasets across training, validation, and test sets.\n",
    "\n",
    "### Plotting Class Distribution Before and After SMOTE\n",
    "\n",
    "The `plot_smote_class_distribution` function plots the class distribution of the training labels before and after applying SMOTE (Synthetic Minority Over-sampling Technique). This visualization helps in understanding how SMOTE balances the class distribution.\n",
    "\n",
    "\n",
    "### Plotting Dataset Distribution\n",
    "\n",
    "The `plot_dataset_distribution` function plots a bar chart showing the sizes of the train, validation, and test sets. This visualization helps in understanding the distribution of samples across different datasets.\n",
    "\n",
    "These methods provide visual insights into the data, which is crucial for understanding and improving the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smote_class_distribution(y_train, y_train_resampled):\n",
    "    \"\"\"\n",
    "    Plots the class distribution before and after applying SMOTE.\n",
    "\n",
    "    Parameters:\n",
    "    - y_train: Original training labels.\n",
    "    - y_train_resampled: Training labels after applying SMOTE.\n",
    "    \"\"\"\n",
    "    # Class distribution before SMOTE\n",
    "    class_distribution_before = Counter(y_train)\n",
    "    # Class distribution after SMOTE\n",
    "    class_distribution_after = Counter(y_train_resampled)\n",
    "\n",
    "    # Define labels\n",
    "    labels = ['No Stress', 'Stress']\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot before SMOTE\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(labels, class_distribution_before.values(), color='blue')\n",
    "    plt.title('Class Distribution Before SMOTE')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], labels)\n",
    "\n",
    "    # Plot after SMOTE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(labels, class_distribution_after.values(), color='green')\n",
    "    plt.title('Class Distribution After SMOTE')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], labels)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_distribution(x_train, y_train, y_val_subject_1, y_val_subject_2, x_test, y_test, feature):\n",
    "    dataset_names = ['Train', 'Test', 'Validation Subject 1', 'Validation Subject 2']\n",
    "    \n",
    "    # Calculate lengths for x and y datasets\n",
    "    x_lengths = [len(x_train[feature]), len(x_test[feature]), len(y_val_subject_1), len(y_val_subject_2)]\n",
    "    y_lengths = [len(y_train), len(y_test), len(y_val_subject_1), len(y_val_subject_2)]\n",
    "    \n",
    "    # Plotting the bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    bar_width = 0.35  # Width of the bars\n",
    "    \n",
    "    index = np.arange(len(dataset_names))\n",
    "    \n",
    "    # Create bar plots for x and y datasets\n",
    "    plt.bar(index, x_lengths, bar_width, color='b', alpha=0.6, label=f'X (Feature: {feature})')\n",
    "    plt.bar(index + bar_width, y_lengths, bar_width, color='r', alpha=0.6, label='Y (Labels)')\n",
    "    \n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Dataset Distribution')\n",
    "    plt.xticks(index + bar_width / 2, dataset_names)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(file_path, chunk_size=100000, usecols=None, dtype=None):\n",
    "    \"\"\"Reads a CSV file in chunks, adds a subject_id column, and returns a list of DataFrames for each chunk.\"\"\"\n",
    "    dataframes = []\n",
    "    try:\n",
    "        # Read the CSV in chunks\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size, usecols=usecols, dtype=dtype):\n",
    "            dataframes.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading chunks from {file_path}: {e}\")\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(subjects, sensors=(\"unknown\"), chunk_size=100000, usecols=None, dtype=None):\n",
    "    \"\"\"Load data from multiple CSV files in chunks and concatenate them into a single DataFrame.\"\"\"\n",
    "    dataframes = []\n",
    "        # Create file paths for each subject and sensor\n",
    "    file_paths = [MAIN_PATH + f\"/data/WESAD/{subject}/{subject}_{sensors}_data.csv\" for subject in subjects]\n",
    "    total_files = len(file_paths)\n",
    "\n",
    "    # Use a thread pool to read files in chunks in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(read_chunk, file_path, chunk_size, usecols, dtype): file_path for file_path in file_paths}\n",
    "\n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            file_path = futures[future]\n",
    "            try:\n",
    "                # Extend dataframes with the result of read_chunk\n",
    "                dataframes.extend(future.result())\n",
    "                print(f\"Completed reading all chunks from {file_path} ({i + 1}/{total_files}).\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunks from {file_path}: {e}\")\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    all_data = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Sort by subject_id and time columns, assuming 'time' column exists in CSVs\n",
    "    return all_data.sort_values(by=[\"ID\", \"StartTime\"]).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_subjects(df):\n",
    "    test_df = df[(df['ID'] == 'S16') | (df['ID'] == 'S17')]\n",
    "\n",
    "    # Drop the filtered rows from the original DataFrame\n",
    "    sequences_df = df.drop(test_df.index)\n",
    "\n",
    "    # Reset index for both DataFrames\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    sequences_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    test_subject_1 = test_df[test_df['ID'] == 'S16']\n",
    "    test_subject_2 = test_df[test_df['ID'] == 'S17']\n",
    "\n",
    "    # Check if the test subjects DataFrames are empty\n",
    "    if test_subject_1.empty:\n",
    "        print(\"Warning: No data found for test subject 1 (ID 16)\")\n",
    "    if test_subject_2.empty:\n",
    "        print(\"Warning: No data found for test subject 2 (ID 17)\")\n",
    "\n",
    "    return test_subject_1, test_subject_2, sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_flatten(series_list):\n",
    "    try:\n",
    "        scaler = MinMaxScaler()\n",
    "        return [scaler.fit_transform(np.asarray(series).reshape(-1, 1)).flatten() for series in series_list]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to scale and flatten series: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics(df, t_df1, t_df2):\n",
    "    try:\n",
    "        eda_array_list = scale_and_flatten(df[METRIC].apply(eval))\n",
    "        test_subject1_array_list = scale_and_flatten(t_df1[METRIC].apply(eval))\n",
    "        test_subject2_array_list = scale_and_flatten(t_df2[METRIC].apply(eval))\n",
    "\n",
    "        # Print counts\n",
    "        print(f\"EDA list Count: {len(eda_array_list)}\\n Test Subject 1: {len(test_subject1_array_list)} \\n Test Subject 2: {len(test_subject2_array_list)} \")\n",
    "        \n",
    "        return eda_array_list, test_subject1_array_list, test_subject2_array_list\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to process metrics: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sequences_df, features):\n",
    "    \"\"\"\n",
    "    Prepare the data by extracting the specified features and labels from the sequences DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sequences_df (DataFrame): DataFrame containing the sequences of sensor data.\n",
    "        features (list): List of features to extract from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of feature arrays and labels (y).\n",
    "    \"\"\"\n",
    "    feature_arrays = {}\n",
    "    \n",
    "    # Extracting features dynamically based on the provided list\n",
    "    for feature in features:\n",
    "        feature_arrays[feature] = np.array([eval(x) for x in sequences_df[feature]])\n",
    "    \n",
    "    # Extracting labels\n",
    "    y = sequences_df['labels'].values\n",
    "    \n",
    "    return feature_arrays, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(X_dict, y):\n",
    "    \"\"\"\n",
    "    Apply SMOTE to balance the dataset.\n",
    "\n",
    "    Args:\n",
    "        X_dict (dict): Dictionary of features, where each feature is a NumPy array of shape (num_samples, sequence_length).\n",
    "        y (array): Labels array.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Oversampled feature dictionary and label array.\n",
    "    \"\"\"\n",
    "    # Flatten each feature to make it compatible with SMOTE\n",
    "    flattened_features = np.hstack([X.reshape(X.shape[0], -1) for X in X_dict.values()])\n",
    "    \n",
    "    # Apply SMOTE to the flattened features and labels\n",
    "    smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(flattened_features, y)\n",
    "    X_resampled = reshape_features(X_resampled, X_dict)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def reshape_features(X_resampled, X_dict):\n",
    "    \"\"\"\n",
    "    Reshape the resampled features back to their original shape.\n",
    "\n",
    "    Args:\n",
    "        X_resampled (array): Resampled feature array.\n",
    "        X_dict (dict): Original dictionary of features.\n",
    "\n",
    "    Returns:\n",
    "        dict: Reshaped feature dictionary.\n",
    "    \"\"\"\n",
    "    start_idx = 0\n",
    "    X_resampled_dict = {}\n",
    "    for feature, X in X_dict.items():\n",
    "        feature_length = X.shape[1] * X.shape[2] if X.ndim == 3 else X.shape[1]\n",
    "        X_resampled_dict[feature] = X_resampled[:, start_idx:start_idx + feature_length].reshape(-1, X.shape[1], X.shape[2] if X.ndim == 3 else 1)\n",
    "        start_idx += feature_length\n",
    "    \n",
    "    return X_resampled_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(feature_arrays, y, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        feature_arrays (dict): Dictionary containing feature arrays.\n",
    "        y (array): Labels array.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Split data arrays for each feature and labels.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to hold training and testing splits\n",
    "    split_data = {}\n",
    "    \n",
    "    # Split the data for each feature in feature_arrays\n",
    "    for feature, data in feature_arrays.items():\n",
    "        split_data[f\"{feature}_train\"], split_data[f\"{feature}_test\"], y_train, y_test = train_test_split(data, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Add the labels to the split data\n",
    "    split_data['y_train'] = y_train\n",
    "    split_data['y_test'] = y_test\n",
    "    \n",
    "    return split_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(df, t_df1, t_df2, label_column='labels'):\n",
    "    try:\n",
    "        # Extract labels for the main dataset and test subjects\n",
    "        labels_array = df['labels'].values\n",
    "        validation_labels_array_subject_1 = t_df1['labels'].values\n",
    "        validation_labels_array_subject_2 = t_df2['labels'].values\n",
    "\n",
    "        print(\n",
    "            f\"Labels list Count Subject 1: {len(validation_labels_array_subject_1)}\\n\"\n",
    "            f\"Labels list Count Subject 2: {len(validation_labels_array_subject_2)}\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        return labels_array, validation_labels_array_subject_1, validation_labels_array_subject_2\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to extract labels: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets_to_pickle(x_train, y_train, x_val, y_val, x_test_1, y_test_1, x_test_2, y_test_2, save_directory=\"../data/results/\"):\n",
    "    \"\"\"Save the datasets to a pickle file.\n",
    "    Args:\n",
    "        x_train (ndarray): Training feature data.\n",
    "        y_train (ndarray): Training label data.\n",
    "        x_val (ndarray): Validation feature data.\n",
    "        y_val (ndarray): Validation label data.\n",
    "        x_test_1 (ndarray): Test feature data for Subject 1.\n",
    "        y_test_1 (ndarray): Test label data for Subject 1.\n",
    "        x_test_2 (ndarray): Test feature data for Subject 2.\n",
    "        y_test_2 (ndarray): Test label data for Subject 2.\n",
    "        filename (str): The name of the file to save the datasets to (should end with .pkl).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Dictionary to store all datasets\n",
    "        datasets = {\n",
    "            \"x_train\": x_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"x_val\": x_val,\n",
    "            \"y_val\": y_val,\n",
    "            \"x_test_1\": x_test_1,\n",
    "            \"y_test_1\": y_test_1,\n",
    "            \"x_test_2\": x_test_2,\n",
    "            \"y_test_2\": y_test_2\n",
    "        }\n",
    "        \n",
    "        # Save each dataset as a separate pickle file\n",
    "        for name, data in datasets.items():\n",
    "            save_path = f\"{save_directory}{name}.pkl\"\n",
    "            with open(save_path, 'wb') as file:\n",
    "                pickle.dump(data, file)\n",
    "            print(f\"{name} saved successfully to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save datasets: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    subjects = [f\"S{i}\" for i in range(2, 18)]\n",
    "    features = ['EDA', 'BVP', 'ACC', 'TEMP']\n",
    "    \n",
    "    # Load the data from CSV files\n",
    "    df = load_data(subjects)\n",
    "    t_df1, t_df2, df = filter_subjects(df)\n",
    "\n",
    "    # Extract features for t_df1 and t_df2\n",
    "    x_val_subject_1 = t_df1[features]\n",
    "    x_val_subject_2 = t_df2[features]\n",
    "    y, y_val_subject_1, y_val_subject_2 = extract_labels(df, t_df1, t_df2)\n",
    "    print(f\"Dataframe shape: {df.shape}, type: {type(df)}\")\n",
    "\n",
    "    # Get and print the shapes of the feature DataFrames\n",
    "    print(f\"t_df1 features shape: {x_val_subject_1.shape}\")\n",
    "    print(f\"t_df2 features shape: {x_val_subject_2.shape}\")\n",
    "\n",
    "    # Prepare the data for training\n",
    "    feature_arrays, y = prepare_data(df, features)\n",
    "    split_data_dict = split_data(feature_arrays, y, test_size=0.2)\n",
    "\n",
    "    # Initialize dictionaries for X and y\n",
    "    x_train = {feature: split_data_dict[f\"{feature}_train\"] for feature in features}\n",
    "    x_test = {feature: split_data_dict[f\"{feature}_test\"] for feature in features}\n",
    "    y_train = split_data_dict[\"y_train\"]\n",
    "    y_test = split_data_dict[\"y_test\"]\n",
    "    \n",
    "    # Apply SMOTE only on the training data\n",
    "    x_train, y_train = apply_smote(x_train, y_train)\n",
    "    x_test, y_test = apply_smote(x_test, y_test)\n",
    "\n",
    "\n",
    "    # Display shapes of resampled data\n",
    "    for feature in features:\n",
    "        print(f\"Resampled train {feature} shape: {x_train[feature].shape}, Test {feature} shape: {x_test[feature].shape}\")\n",
    "    print(f\"Resampled train labels shape: {y_train.shape}, Test labels shape: {y_test.shape}\")\n",
    "\n",
    "    for feature in features:\n",
    "        print(f\"x_val_subject_1 {feature} shape: {x_val_subject_1[feature].shape}, x_val_subject_2 {feature} shape: {x_val_subject_2[feature].shape}\")\n",
    "    print(f\"y_val1: {y_val_subject_1.shape}, y_val2 shape: {y_val_subject_2.shape}\")\n",
    "\n",
    "    plot_dataset_distribution(x_train, y_train, y_val_subject_1, y_val_subject_2, x_test, y_test, 'EDA')\n",
    "    # save_datasets_to_pickle(x_train, y_train, x_test, y_test, x_val_subject_1, y_val_subject_1, x_val_subject_2, y_val_subject_2)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_val_subject_1, y_val_subject_1, x_val_subject_2, y_val_subject_2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading chunks from c:\\Master of Applied IT/data/WESAD/S12/S12_unknown_data.csv: [Errno 2] No such file or directory: 'c:\\\\Master of Applied IT/data/WESAD/S12/S12_unknown_data.csv'\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S12/S12_unknown_data.csv (1/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S7/S7_unknown_data.csv (2/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S8/S8_unknown_data.csv (3/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S2/S2_unknown_data.csv (4/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S3/S3_unknown_data.csv (5/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S9/S9_unknown_data.csv (6/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S11/S11_unknown_data.csv (7/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S13/S13_unknown_data.csv (8/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S5/S5_unknown_data.csv (9/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S4/S4_unknown_data.csv (10/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S14/S14_unknown_data.csv (11/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S10/S10_unknown_data.csv (12/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S6/S6_unknown_data.csv (13/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S15/S15_unknown_data.csv (14/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S16/S16_unknown_data.csv (15/16).\n",
      "Completed reading all chunks from c:\\Master of Applied IT/data/WESAD/S17/S17_unknown_data.csv (16/16).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to extract labels: 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'labels'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 4\u001b[0m, in \u001b[0;36mextract_labels\u001b[1;34m(df, t_df1, t_df2, label_column)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Extract labels for the main dataset and test subjects\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     labels_array \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      5\u001b[0m     validation_labels_array_subject_1 \u001b[38;5;241m=\u001b[39m t_df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'labels'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train, y_train, x_test, y_test, x_val_subject_1, y_val_subject_1, x_val_subject_2, y_val_subject_2 \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[181], line 12\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m x_val_subject_1 \u001b[38;5;241m=\u001b[39m t_df1[features]\n\u001b[0;32m     11\u001b[0m x_val_subject_2 \u001b[38;5;241m=\u001b[39m t_df2[features]\n\u001b[1;32m---> 12\u001b[0m y, y_val_subject_1, y_val_subject_2 \u001b[38;5;241m=\u001b[39m \u001b[43mextract_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_df1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_df2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataframe shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get and print the shapes of the feature DataFrames\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[119], line 16\u001b[0m, in \u001b[0;36mextract_labels\u001b[1;34m(df, t_df1, t_df2, label_column)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels_array, validation_labels_array_subject_1, validation_labels_array_subject_2\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to extract labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to extract labels: 'labels'"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, x_val_subject_1, y_val_subject_1, x_val_subject_2, y_val_subject_2 = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA</th>\n",
       "      <th>BVP</th>\n",
       "      <th>ACC</th>\n",
       "      <th>TEMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.00477192755621364, 0.0042414831556966265, 0...</td>\n",
       "      <td>[0.4749997489370076, 0.4738879087743026, 0.472...</td>\n",
       "      <td>[0.6614658429145402, 0.19968450351717293, 0.43...</td>\n",
       "      <td>[0.053763440860215894, 0.053763440860215894, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.006362431938388882, 0.006892461929217997, 0...</td>\n",
       "      <td>[0.4732267814461861, 0.4779190052961181, 0.481...</td>\n",
       "      <td>[0.2674741078266569, 0.2674741078266569, 0.267...</td>\n",
       "      <td>[0.07526881720430012, 0.07526881720430012, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.006362431938388882, 0.006362431938388882, 0...</td>\n",
       "      <td>[0.4692837313840229, 0.4676887293066898, 0.465...</td>\n",
       "      <td>[0.26979974915973287, 0.26916389827672244, 0.2...</td>\n",
       "      <td>[0.08602150537634401, 0.08602150537634401, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.006362431938388882, 0.00742290632973501, 0....</td>\n",
       "      <td>[0.4817273946043637, 0.4817327503516943, 0.481...</td>\n",
       "      <td>[0.2674741078266569, 0.2668332026528418, 0.266...</td>\n",
       "      <td>[0.08602150537634401, 0.08602150537634401, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.009013410711910252, 0.011133945084914637, 0...</td>\n",
       "      <td>[0.4806812796434916, 0.48188739278990156, 0.48...</td>\n",
       "      <td>[0.26800086209830015, 0.27509518200431665, 0.2...</td>\n",
       "      <td>[0.06451612903225801, 0.06451612903225801, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 EDA  \\\n",
       "0  [0.00477192755621364, 0.0042414831556966265, 0...   \n",
       "1  [0.006362431938388882, 0.006892461929217997, 0...   \n",
       "2  [0.006362431938388882, 0.006362431938388882, 0...   \n",
       "3  [0.006362431938388882, 0.00742290632973501, 0....   \n",
       "4  [0.009013410711910252, 0.011133945084914637, 0...   \n",
       "\n",
       "                                                 BVP  \\\n",
       "0  [0.4749997489370076, 0.4738879087743026, 0.472...   \n",
       "1  [0.4732267814461861, 0.4779190052961181, 0.481...   \n",
       "2  [0.4692837313840229, 0.4676887293066898, 0.465...   \n",
       "3  [0.4817273946043637, 0.4817327503516943, 0.481...   \n",
       "4  [0.4806812796434916, 0.48188739278990156, 0.48...   \n",
       "\n",
       "                                                 ACC  \\\n",
       "0  [0.6614658429145402, 0.19968450351717293, 0.43...   \n",
       "1  [0.2674741078266569, 0.2674741078266569, 0.267...   \n",
       "2  [0.26979974915973287, 0.26916389827672244, 0.2...   \n",
       "3  [0.2674741078266569, 0.2668332026528418, 0.266...   \n",
       "4  [0.26800086209830015, 0.27509518200431665, 0.2...   \n",
       "\n",
       "                                                TEMP  \n",
       "0  [0.053763440860215894, 0.053763440860215894, 0...  \n",
       "1  [0.07526881720430012, 0.07526881720430012, 0.0...  \n",
       "2  [0.08602150537634401, 0.08602150537634401, 0.0...  \n",
       "3  [0.08602150537634401, 0.08602150537634401, 0.0...  \n",
       "4  [0.06451612903225801, 0.06451612903225801, 0.0...  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_subject_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.23784528],\n",
       "        [0.23653046],\n",
       "        [0.23784528],\n",
       "        ...,\n",
       "        [0.23258905],\n",
       "        [0.23653046],\n",
       "        [0.23653046]],\n",
       "\n",
       "       [[0.0457014 ],\n",
       "        [0.04619038],\n",
       "        [0.04599482],\n",
       "        ...,\n",
       "        [0.05251887],\n",
       "        [0.05183425],\n",
       "        [0.05242109]],\n",
       "\n",
       "       [[0.20001794],\n",
       "        [0.2001502 ],\n",
       "        [0.19948852],\n",
       "        ...,\n",
       "        [0.19670917],\n",
       "        [0.1963121 ],\n",
       "        [0.19617975]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.01668112],\n",
       "        [0.0168004 ],\n",
       "        [0.01668112],\n",
       "        ...,\n",
       "        [0.01695123],\n",
       "        [0.01708623],\n",
       "        [0.0165304 ]],\n",
       "\n",
       "       [[0.56264291],\n",
       "        [0.55940605],\n",
       "        [0.55714796],\n",
       "        ...,\n",
       "        [0.54310592],\n",
       "        [0.54734582],\n",
       "        [0.54701973]],\n",
       "\n",
       "       [[0.5319962 ],\n",
       "        [0.5319962 ],\n",
       "        [0.53318354],\n",
       "        ...,\n",
       "        [0.53933201],\n",
       "        [0.53792937],\n",
       "        [0.53965625]]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['EDA']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

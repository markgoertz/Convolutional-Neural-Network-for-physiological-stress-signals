{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data/unprocessed/WESAD\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_in_chunks(file_path, subject_id, chunksize=100000):\n",
    "    \"\"\"Load data in chunks, assign ID and return concatenated DataFrame.\"\"\"\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        chunk['ID'] = subject_id  # Assign subject ID to each chunk\n",
    "        chunks.append(chunk)  # Collect the chunk\n",
    "    return pd.concat(chunks, ignore_index=True)  # Concatenate all chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def save_batch(batch_df, output_file):\n",
    "    \"\"\"\n",
    "    Saves a batch of the DataFrame to a single file in append mode.\n",
    "    \"\"\"\n",
    "    batch_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "\n",
    "def save_dataframe_in_batches(df, output_path, id_column='ID', batch_size=100):\n",
    "    unique_ids = df[id_column].unique()\n",
    "    total_ids = len(unique_ids)  # Total unique IDs\n",
    "    total_batches = total_ids // batch_size + (total_ids % batch_size > 0)\n",
    "\n",
    "    processed_ids = 0  # Counter for processed IDs\n",
    "    print(f'Starting saving dataframe in batches. Total IDs: {total_ids}. Total batches: {total_batches}.')\n",
    "    \n",
    "    # Define the output file path\n",
    "    output_file = f\"{output_path}.csv\"\n",
    "    \n",
    "    # Remove the existing output file if it exists (for fresh run)\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch_number in range(total_batches):\n",
    "            batch_ids = unique_ids[batch_number * batch_size:(batch_number + 1) * batch_size]\n",
    "            \n",
    "            # Filter the DataFrame for the current batch of IDs\n",
    "            batch_df = df[df[id_column].isin(batch_ids)]\n",
    "\n",
    "            # Submit the batch for saving using parallel processing\n",
    "            futures.append(executor.submit(save_batch, batch_df, output_file))\n",
    "\n",
    "            # Notify user of progress\n",
    "            print(f\"Processed batch {batch_number + 1}/{total_batches}.\")\n",
    "            for current_id in batch_ids:\n",
    "                processed_ids += 1\n",
    "                print(f'Processed ID: {current_id} ({processed_ids}/{total_ids})')\n",
    "\n",
    "        # Ensure all tasks are completed before proceeding\n",
    "        for future in as_completed(futures):\n",
    "            future.result()  # This will raise an exception if any occurred in the threads\n",
    "\n",
    "    print(f\"Finished saving dataframe. Output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(df, strategy='drop', fill_value=None):\n",
    "    if strategy == 'drop':\n",
    "        return df.dropna()\n",
    "    elif strategy == 'fill':\n",
    "        return df.fillna(fill_value)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported missing data strategy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxEDA.src.cvxEDA\n",
    "\n",
    "def calculate_eda_levels(y):\n",
    "    fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700}\n",
    "    Fs = fs_dict['EDA']\n",
    "    yn = (y - y.mean()) / y.std()\n",
    "    r, p, t, l, d, e, obj = cvxEDA.src.cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "    return r, t, yn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    # Calculate EDA levels\n",
    "    unique_ids = dataset['ID'].unique()\n",
    "    new_dataframe_eda = pd.DataFrame(columns=[\"cvx_phasic\", \"cvx_tonic\"])\n",
    "    \n",
    "    for unique_id in unique_ids:\n",
    "        subset_data = dataset[dataset['ID'] == unique_id]\n",
    "        phasic, tonic, yn = calculate_eda_levels(subset_data['EDA'].values)\n",
    "        temp_df = pd.DataFrame({\"cvx_phasic\": phasic, \"cvx_tonic\": tonic})\n",
    "        new_dataframe_eda = pd.concat([new_dataframe_eda, temp_df], ignore_index=True)\n",
    "\n",
    "    dataset = pd.concat([dataset, new_dataframe_eda], axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_wrist_sequences_df(merged_df, max_length=32):\n",
    "    sequences = []\n",
    "    \n",
    "    # Initialize storage for the current sequence\n",
    "    current_sequence = defaultdict(list)\n",
    "    current_id = None\n",
    "    \n",
    "    for index, row in merged_df.iterrows():\n",
    "        if current_id != row['ID']:\n",
    "            # Store the previous sequence if it exists\n",
    "            if current_id is not None:\n",
    "                sequences.append({\n",
    "                    'ID': current_id,\n",
    "                    'EDA': current_sequence['EDA'],\n",
    "                    'TEMP': current_sequence['TEMP'],\n",
    "                    'labels': current_sequence['labels'],\n",
    "                    'Time': current_sequence['Time'],\n",
    "                    'X': current_sequence['X'],\n",
    "                    'Y': current_sequence['Y'],\n",
    "                    'Z': current_sequence['Z'],\n",
    "                    'BVP': current_sequence['BVP']\n",
    "                })\n",
    "            \n",
    "            # Reset for new ID\n",
    "            current_id = row['ID']\n",
    "            current_sequence = defaultdict(list)\n",
    "            current_sequence['labels'] = row['labels']\n",
    "        \n",
    "        # Append values to the current sequence\n",
    "        current_sequence['TEMP'].append(row['TEMP'])\n",
    "        current_sequence['EDA'].append(row['EDA'])\n",
    "        current_sequence['Time'].append(row['Time'])\n",
    "        current_sequence['X'].append(row['X'])\n",
    "        current_sequence['Y'].append(row['Y'])\n",
    "        current_sequence['Z'].append(row['Z'])\n",
    "        current_sequence['BVP'].append(row['BVP'])\n",
    "\n",
    "        # Check if sequence length exceeds max_length\n",
    "        if len(current_sequence['TEMP']) >= max_length:\n",
    "            sequences.append({\n",
    "                'ID': current_id,\n",
    "                'EDA': current_sequence['EDA'],\n",
    "                'TEMP': current_sequence['TEMP'],\n",
    "                'labels': current_sequence['labels'],\n",
    "                'Time': current_sequence['Time'],\n",
    "                'X': current_sequence['X'],\n",
    "                'Y': current_sequence['Y'],\n",
    "                'Z': current_sequence['Z'],\n",
    "                'BVP': current_sequence['BVP']\n",
    "            })\n",
    "            # Reset the current sequence for further accumulation\n",
    "            current_sequence = defaultdict(list)\n",
    "            current_sequence['labels'] = row['labels']\n",
    "\n",
    "    # Append the last sequence if it's not empty\n",
    "    if current_sequence['TEMP']:\n",
    "        sequences.append({\n",
    "            'ID': current_id,\n",
    "            'EDA': current_sequence['EDA'],\n",
    "            'TEMP': current_sequence['TEMP'],\n",
    "            'labels': current_sequence['labels'],\n",
    "            'Time': current_sequence['Time'],\n",
    "            'X': current_sequence['X'],\n",
    "            'Y': current_sequence['Y'],\n",
    "            'Z': current_sequence['Z'],\n",
    "            'BVP': current_sequence['BVP']\n",
    "        })\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    sequences_df = pd.DataFrame(sequences)\n",
    "    return sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_chest_sequences_df(merged_df, max_length=32):\n",
    "    sequences = []\n",
    "    total_ids = merged_df['ID'].nunique()  # Get the total number of unique IDs\n",
    "    processed_ids = 0  # Initialize a counter for processed IDs\n",
    "\n",
    "    # Group by ID\n",
    "    grouped = merged_df.groupby('ID')\n",
    "\n",
    "    for current_id, group in grouped:\n",
    "        # Convert group to NumPy arrays for better performance\n",
    "        temp = group['TEMP'].to_numpy()\n",
    "        eda = group['EDA'].to_numpy()\n",
    "        time = group['Time'].to_numpy()\n",
    "        x = group['X'].to_numpy()\n",
    "        y = group['Y'].to_numpy()\n",
    "        z = group['Z'].to_numpy()\n",
    "        ecg = group['ECG'].to_numpy()\n",
    "        emg = group['EMG'].to_numpy()\n",
    "        resp = group['RESP'].to_numpy()\n",
    "        label = group['labels'].iloc[0]  # Assuming labels are consistent within the group\n",
    "\n",
    "        # Initialize current sequence lengths and start index\n",
    "        current_length = len(temp)\n",
    "        start_idx = 0\n",
    "        \n",
    "        while start_idx < current_length:\n",
    "            end_idx = min(start_idx + max_length, current_length)  # Define end index\n",
    "            \n",
    "            sequences.append({\n",
    "                'ID': current_id,\n",
    "                'EDA': eda[start_idx:end_idx].tolist(),\n",
    "                'TEMP': temp[start_idx:end_idx].tolist(),\n",
    "                'labels': label,\n",
    "                'Time': time[start_idx:end_idx].tolist(),\n",
    "                'X': x[start_idx:end_idx].tolist(),\n",
    "                'Y': y[start_idx:end_idx].tolist(),\n",
    "                'Z': z[start_idx:end_idx].tolist(),\n",
    "                'ECG': ecg[start_idx:end_idx].tolist(),\n",
    "                'EMG': emg[start_idx:end_idx].tolist(),\n",
    "                'RESP': resp[start_idx:end_idx].tolist()\n",
    "            })\n",
    "\n",
    "            start_idx += max_length  # Move to the next batch\n",
    "        \n",
    "        # Update the processed IDs counter\n",
    "        processed_ids += 1\n",
    "        print(f'Processed ID: {current_id} ({processed_ids}/{total_ids})')  # Progress update\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    sequences_df = pd.DataFrame(sequences)\n",
    "    print('Finished creating chest sequences DataFrame.')  # Final notification\n",
    "    return sequences_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(df, encoding_type='labels'):\n",
    "    df_encoded = df.copy()\n",
    "    df_encoded['labels'] = df['labels'].apply(lambda x: 1 if x == 2 else 0)\n",
    "\n",
    "    if encoding_type == 'labels':\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(df_encoded['labels'])\n",
    "        return df_encoded\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported encoding type!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(categorical_columns=None):\n",
    "    print(f'Starting loading data')\n",
    "    \n",
    "    wrist_data_list = []\n",
    "    chest_data_list = []\n",
    "    \n",
    "    for subject_id in range(2, 18):\n",
    "        if subject_id == 12:\n",
    "            continue  # Skip subject 12 as per your logic\n",
    "        \n",
    "        wrist_file_path = os.path.join(DATA_PATH, f'S{subject_id}', 'wrist', f'S{subject_id}.csv')\n",
    "        # chest_file_path = os.path.join(DATA_PATH, f'S{subject_id}', 'chest', f'S{subject_id}.csv')\n",
    "        \n",
    "        # Load data in chunks and concatenate\n",
    "        wrist_data = load_data_in_chunks(wrist_file_path, subject_id)\n",
    "        # chest_data = load_data_in_chunks(chest_file_path, subject_id)\n",
    "\n",
    "        # Append to the list\n",
    "        wrist_data_list.append(wrist_data)\n",
    "        # chest_data_list.append(chest_data)\n",
    "\n",
    "    print(f'Finished loading data')\n",
    "\n",
    "    # Concatenate all wrist and chest data into single DataFrames\n",
    "    wrist_data_df = pd.concat(wrist_data_list, ignore_index=True)\n",
    "    # chest_data_df = pd.concat(chest_data_list, ignore_index=True)\n",
    "\n",
    "    print(f'Starting preprocessing')\n",
    "    \n",
    "    # Process the data\n",
    "    wrist_sequences_df = create_wrist_sequences_df(wrist_data_df)\n",
    "    print(f\"wrist preprocessing\")\n",
    "    # chest_sequences_df = create_chest_sequences_df(chest_data_df)\n",
    "\n",
    "    print(f'Finished creating sequences')\n",
    "\n",
    "    # Encode categorical columns\n",
    "    result_wrist_df = encode_categorical(wrist_sequences_df)\n",
    "    # result_chest_df = encode_categorical(chest_sequences_df)\n",
    "    print(f'Finished encoding')\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = '../data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save DataFrames in batches\n",
    "    # save_dataframe_in_batches(result_chest_df, output_path='../data/chest_result_df', id_column='ID', batch_size=100)\n",
    "    save_dataframe_in_batches(result_wrist_df, output_path='../data/wrist_result_df', id_column='ID', batch_size=100)\n",
    "\n",
    "    return result_wrist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loading data\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m x\n",
      "Cell \u001b[1;32mIn[37], line 18\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(categorical_columns)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Load data in chunks and concatenate\u001b[39;00m\n\u001b[0;32m     17\u001b[0m wrist_data \u001b[38;5;241m=\u001b[39m load_data_in_chunks(wrist_file_path, subject_id)\n\u001b[1;32m---> 18\u001b[0m chest_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_in_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchest_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Append to the list\u001b[39;00m\n\u001b[0;32m     21\u001b[0m wrist_data_list\u001b[38;5;241m.\u001b[39mappend(wrist_data)\n",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m, in \u001b[0;36mload_data_in_chunks\u001b[1;34m(file_path, subject_id, chunksize)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load data in chunks, assign ID and return concatenated DataFrame.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, chunksize\u001b[38;5;241m=\u001b[39mchunksize):\n\u001b[0;32m      5\u001b[0m     chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subject_id  \u001b[38;5;66;03m# Assign subject ID to each chunk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(chunk)  \u001b[38;5;66;03m# Collect the chunk\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1984\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[1;32m-> 1985\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "\n",
    "x = main()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import decimate\n",
    "\n",
    "# BVP signal data (64 Hz sampling rate)\n",
    "bvp_data = np.array([\n",
    "    -0.00, -0.00, -0.00, -0.00, -0.00, -0.00, -0.00, -0.00, -0.00, -0.00,\n",
    "    0.00, 0.01, 0.01, -0.00, -0.03, -0.05, -0.05, 0.00, 0.13, 0.36,\n",
    "    0.66, 1.01, 1.37, 1.72, 2.06, 2.40, 2.79, 3.24, 3.80, 4.42,\n",
    "    5.06, 5.66, 6.20, 6.68, 7.15, 7.68, 8.31, 9.14, 10.09, 11.04,\n",
    "    11.88, 12.56, 13.14, 13.76, 14.65, 15.96, 17.91, 20.32, 22.90, 25.38,\n",
    "    27.58, 29.57, 31.68, 34.32, 37.81, 42.54, 48.15, 54.10, 59.83, 64.97,\n",
    "    69.58, 74.14, 79.34, 85.71, 93.76, 103.00, 112.62, 121.73, 129.77, 136.75,\n",
    "    143.30, 150.75, 157.77, 166.64, 177.95, 191.79, 206.97, 220.57, 228.25, 225.36,\n",
    "    208.53, 176.87, 132.76, 80.05, 23.07, -34.94, -92.44, -149.16, -205.23,\n",
    "    -260.37, -313.74, -363.87, -409.83, -450.91, -486.50, -515.74, -537.23,\n",
    "    -549.22, -549.96, -538.37, -514.21, -478.67, -433.88, -382.45, -327.02,\n",
    "    -269.84, -212.63, -156.66, -102.95, -52.37, -5.90, 35.58, 71.43,\n",
    "    101.35, 125.50, 144.38, 158.69, 169.15, 180.75, 182.57, 182.07,\n",
    "    179.51, 175.23, 169.65, 163.18, 156.15, 148.81, 141.30, 133.69,\n",
    "    126.02, 118.33, 110.69, 103.16, 95.80, 88.68, 81.83, 75.25,\n",
    "    68.95, 62.95, 57.25, 51.86, 46.78, 42.03, 37.59, 33.45,\n",
    "    29.61, 26.07, 22.81, 19.83, 17.11, 14.67, 12.48, 10.54,\n",
    "    8.83, 7.35, 6.07, 4.97, 4.05, 3.27, 2.62, 2.09,\n",
    "    1.65, 1.30, 1.01, 0.79, 0.61, 0.48, 0.37, 0.29,\n",
    "    0.24, 0.20, 0.18, 0.17, 0.17, 0.18, 0.20, 0.22,\n",
    "    0.26, 0.29, 0.33, 0.37, 0.42, 0.46, 0.51, 0.56,\n",
    "    0.61, 0.66, 0.72, 0.78, 0.84, 0.92, 1.00, 1.08,\n",
    "    1.18, 1.29, 1.42, 1.55, 1.71, 1.88, 2.07, 2.28,\n",
    "    2.51, 2.77, 3.06, 3.37, 3.72, 4.10, 4.51, 4.96,\n",
    "    5.44, 5.96, 6.52, 7.12, 7.79, 8.51, 9.27, 10.06,\n",
    "    10.84, 11.61, 12.37, 13.15, 14.01, 14.97, 16.07, 17.28,\n",
    "    18.56, 19.81, 20.97, 21.95, 22.69, 23.14, 23.24, 22.89,\n",
    "    21.95, 20.31, 17.85, 14.56, 10.47, 5.85, 0.99, -3.76,\n",
    "    -8.15, -12.06, -15.53, -18.65, -21.51, -24.09, -26.20, -27.58,\n",
    "    -28.01, -27.48, -26.30, -25.00, -24.15, -24.16, -25.02, -26.36,\n",
    "    -27.48, -27.77, -26.97, -25.34, -23.61, -22.65, -23.06, -24.88,\n",
    "    -27.48, -29.91, -31.33, -31.36, -30.31, -28.99, -28.34, -29.23,\n",
    "    -32.31, -37.87, -45.61, -54.13, -60.77, -62.03, -54.86, -38.11,\n",
    "    -13.59, 14.22, 39.73, 58.35, 68.10, 69.90, 66.53, 60.93,\n",
    "    54.97, 49.20, 43.45, 37.66, 32.30, 28.27, 26.25, 26.26,\n",
    "    27.59, 29.18, 29.99, 29.54, 28.06, 26.12, 24.38, 23.19,\n",
    "    22.42, 21.48, 19.61, 16.28, 11.39, 5.38, -1.05, -7.26,\n",
    "    -12.96, -18.27, -23.50, -28.90, -34.47, -39.98, -45.24, -50.41,\n",
    "    -55.95, -62.50, -70.37, -79.28, -88.05, -94.85, -97.61, -94.67,\n",
    "    -85.37, -70.18, -50.62, -28.78, -6.82, 13.49, 30.94, 44.95,\n",
    "    55.42, 62.54, 66.73, 68.49, 68.35, 66.83, 64.36, 61.35,\n",
    "    58.13, 55.00, 52.17, 49.72, 47.66, 45.99, 44.77, 44.12,\n",
    "    44.12, 44.65, 45.36, 45.64, 44.93, 42.85, 39.41, 34.90,\n",
    "    29.77, 24.44, 19.08, 13.64, 7.87, 1.43, -5.95, -14.47,\n",
    "    -24.06, -34.43, -45.02, -55.19, -64.43, -72.50, -79.63, -86.45,\n",
    "    -93.81, -102.50, -112.90, -124.67, -136.71, -147.25, -154.25, -155.92,\n",
    "    -151.12, -139.77, -122.82, -102.02, -79.50, -57.16, -36.27, -17.28,\n",
    "    0.01, 16.18, 31.72, 46.81, 61.23, 74.50, 86.15, 95.88,\n",
    "    103.61, 109.35, 113.08, 114.78, 114.49, 112.53, 109.47, 106.08,\n",
    "    103.03, 100.60, 98.64, 96.58, 93.77, 89.70, 84.23, 77.65,\n",
    "    70.48, 63.23, 56.21, 49.42, 42.59, 35.39, 27.55, 19.05,\n",
    "    10.05, 0.86, -8.16, -16.66, -24.36, -31.13, -37.04, -42.46,\n",
    "    -48.06, -54.54, -62.36, -71.35, -80.52, -88.17, -92.32, -91.30,\n",
    "    -84.30, -71.65, -54.72, -35.54, -16.19, 1.63, 16.83, 28.96,\n",
    "    37.96, 44.07, 47.57, 48.79, 48.10, 45.90, 42.69, 39.02,\n",
    "    35.43, 32.34, 29.97, 28.32, 27.24, 26.54, 26.11, 25.94,\n",
    "    26.04, 26.35, 26.67, 26.69, 26.06, 24.59, 22.27, 19.25,\n",
    "    15.72, 11.79, 7.45, 2.53, -3.10, -9.43, -16.29, -23.38,\n",
    "    -30.44, -37.40, -44.45, -52.01, -60.42, -69.68, -79.07, -87.18,\n",
    "    -92.15, -92.29, -86.56, -75.05, -58.92, -40.05, -20.46, -1.82,\n",
    "    14.82, 28.90, 40.29, 49.05, 55.29, 59.21, 61.14, 61.40,\n",
    "    60.35, 58.28, 55.42, 51.97, 48.14, 44.17, 40.25, 36.58,\n",
    "    33.32, 30.60, 28.50, 26.95, 25.79, 24.72, 23.39, 21.60,\n",
    "    19.28, 16.45, 13.11, 9.07, 4.09, -1.95, -8.89, -16.19,\n",
    "    -23.18, -29.40, -34.88, -40.10, -45.85, -52.73, -60.96, -70.35,\n",
    "    -80.29, -89.94, -98.22, -103.81, -105.28, -101.45, -91.75, -76.53,\n",
    "    -57.05, -35.14, -12.73, 8.53, 27.48, 43.37, 55.85, 64.81,\n",
    "    70.41, 73.03, 73.24, 71.64, 68.75, 65.00, 60.71, 56.26,\n",
    "    52.07, 48.63, 46.30, 45.17, 44.96, 45.13, 45.09, 44.39,\n",
    "    42.92, 40.87, 38.57, 36.28, 34.08, 31.81, 29.25, 26.26,\n",
    "    22.81, 18.97, 14.83, 10.31, 5.18, -0.85, -7.99, -16.19,\n",
    "    -25.15, -34.36, -43.33, -51.72, -59.27, -65.69, -70.43, -72.72,\n",
    "    -71.70, -66.84, -58.13, -46.32, -32.61, -18.40, -4.91, 6.97,\n",
    "    16.71, 24.10, 29.13, 32.09, 33.37, 33.43, 32.65, 31.18,\n",
    "    29.02, 26.07, 22.43, 18.33, 14.14, 10.20, 6.80, 4.11,\n",
    "    2.20, 1.07, 0.55, 0.27, -0.41, -2.12, -5.27, -9.83,\n",
    "    -15.27, -20.81, -25.81, -30.08, -33.98, -38.32, -43.99, -51.53,\n",
    "    -60.90, -71.38, -81.65, -90.14, -95.25, -95.76, -91.06, -81.26,\n",
    "    -67.18, -50.23, -32.10, -14.44, 1.39, 14.57, 24.84, 32.43,\n",
    "    37.83, 41.59, 44.05, 45.36, 45.46, 44.33, 42.04, 38.85,\n",
    "    35.03, 30.95, 26.88, 23.09, 19.86, 17.43, 15.88, 14.97,\n",
    "    14.11, 12.59, 9.93, 6.04, 1.28, -3.82, -8.95, -14.15,\n",
    "    -19.75, -26.06, -33.18, -40.95, -49.23, -57.97, -67.28, -77.02,\n",
    "    -86.46, -94.12, -98.06, -96.54, -88.80, -75.44, -58.24, -39.67,\n",
    "    -21.99, -6.74, 5.51, 15.00, 22.32, 28.01, 32.33, 35.34\n",
    "])\n",
    "\n",
    "bvp_data_normalized = (bvp_data - np.min(bvp_data)) / (np.max(bvp_data) - np.min(bvp_data))\n",
    "\n",
    "# Create a time vector for the original signal\n",
    "fs_original = 64  # original sampling frequency\n",
    "t = np.linspace(0, len(bvp_data_normalized) / fs_original, len(bvp_data_normalized), endpoint=False)\n",
    "\n",
    "# Downsample without filtering (simple slicing)\n",
    "downsampling_factor = 2\n",
    "t_no_filter = t[::downsampling_factor]\n",
    "bvp_no_filter = bvp_data_normalized[::downsampling_factor]\n",
    "\n",
    "# Downsample using the decimate function (with filtering)\n",
    "bvp_decimated = decimate(bvp_data_normalized, downsampling_factor)\n",
    "\n",
    "# New time vector for the decimated signal\n",
    "t_decimated = np.linspace(0, len(bvp_decimated) / (fs_original / downsampling_factor), len(bvp_decimated), endpoint=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Original Signal\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(t, bvp_data_normalized, label='Original Signal (64 Hz)', color='blue')\n",
    "plt.title('Original BVP Signal at 64 Hz')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Downsampled without Decimate (simple slicing)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.stem(t_no_filter, bvp_no_filter, label='Downsampled without Decimate (32 Hz)', linefmt='orange', markerfmt='ro', basefmt=\" \")\n",
    "plt.title('Downsampled BVP Signal without Filtering (32 Hz)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Downsampled with Decimate\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(t_decimated, bvp_decimated, label='Downsampled with Decimate (32 Hz)', color='green')\n",
    "plt.title('Downsampled BVP Signal with Decimate (32 Hz)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

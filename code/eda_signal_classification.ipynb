{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn -q\n",
    "%pip install pandas -q\n",
    "%pip install numpy -q\n",
    "%pip install matplotlib -q\n",
    "%pip install seaborn -q\n",
    "%pip install keras -q\n",
    "%pip install os -q\n",
    "\n",
    "%pip install cvxopt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing, model_selection\n",
    "import random\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cvxEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data/\"\n",
    "\n",
    "QUALITY_THRESHOLD = 128\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(DATA_PATH + \"/merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to create sequences DataFrame\n",
    "def create_sequences_df(dataset, max_length=128):\n",
    "    sequences = []\n",
    "    temp_sequence = []\n",
    "    eda_sequence = []\n",
    "    label = None\n",
    "    time_sequence = []\n",
    "    current_id = None\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        if current_id != row['ID']:\n",
    "            # New ID encountered, append previous sequence to list\n",
    "            if temp_sequence:\n",
    "                sequences.append({\n",
    "                    'ID': current_id,\n",
    "                    'w_eda': eda_sequence,\n",
    "                    'w_temp': temp_sequence,\n",
    "                    'downsampled_label': label,\n",
    "                    'Time': time_sequence\n",
    "                })\n",
    "            # Reset sequences for new ID\n",
    "            temp_sequence = [row['w_temp']]\n",
    "            eda_sequence = [row['w_eda']]\n",
    "            label = row['downsampled_labels']\n",
    "            time_sequence = [row['Time']]\n",
    "            current_id = row['ID']\n",
    "        else:\n",
    "            # Append values to sequences\n",
    "            temp_sequence.append(row['w_temp'])\n",
    "            eda_sequence.append(row['w_eda'])\n",
    "            time_sequence.append(row['Time'])\n",
    "\n",
    "        # Check if sequence length exceeds max_length\n",
    "        if len(temp_sequence) >= max_length:\n",
    "            sequences.append({\n",
    "                'ID': current_id,\n",
    "                'w_eda': eda_sequence,\n",
    "                'w_temp': temp_sequence,\n",
    "                'downsampled_label': label,\n",
    "                'Time': time_sequence\n",
    "            })\n",
    "            # Reset sequences for new ID\n",
    "            temp_sequence = []\n",
    "            eda_sequence = []\n",
    "            label = None\n",
    "            time_sequence = []\n",
    "            current_id = None\n",
    "\n",
    "    # Append last sequence if it's not empty\n",
    "    if temp_sequence:\n",
    "        sequences.append({\n",
    "            'ID': current_id,\n",
    "            'w_eda': eda_sequence,\n",
    "            'w_temp': temp_sequence,\n",
    "            'downsampled_label': label,\n",
    "            'Time': time_sequence\n",
    "        })\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    sequences_df = pd.DataFrame(sequences)\n",
    "    return sequences_df\n",
    "\n",
    "# Create sequences DataFrame\n",
    "sequences_df = create_sequences_df(dataset)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences_df.loc[88, 'w_eda'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 8, figsize=(25, 6))  # Increased figure size\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define unique_ids\n",
    "unique_ids = dataset['ID'].unique()\n",
    "\n",
    "# Iterate through each unique id\n",
    "for i, unique_id in enumerate(unique_ids):\n",
    "    if i < len(unique_ids):\n",
    "        # Filter data for each id\n",
    "        subset_data = dataset[dataset['ID'] == unique_id]\n",
    "        \n",
    "        # Plotting\n",
    "        sns.lineplot(x='Time', y='w_eda', data=subset_data, ax=axes[i], color='blue', label='EDA')\n",
    "        # sns.lineplot(x='Time', y='w_temp', data=subset_data, ax=axes[i], color='red', label='Temp')\n",
    "\n",
    "        axes[i].set_title(f\"Data for {unique_id}\")\n",
    "        axes[i].set_xlabel('Time')\n",
    "        axes[i].set_ylabel('Measurement')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create subplots\n",
    "# fig, axes = plt.subplots(16, 3, figsize=(60, 40))  # Increased figure size\n",
    "\n",
    "# # Define colors for each acceleration component\n",
    "# colors = ['red', 'green', 'blue']\n",
    "\n",
    "# # Iterate through each unique id\n",
    "# for i, unique_id in enumerate(unique_ids):\n",
    "#     # Filter data for each id\n",
    "#     subset_data = dataset[dataset['ID'] == unique_id]\n",
    "    \n",
    "#     # Iterate through X, Y, and Z accelerations\n",
    "#     for j, accel_component in enumerate(['X', 'Y', 'Z']):\n",
    "#         ax = axes[i, j]  # Select the appropriate subplot\n",
    "        \n",
    "#         # Plot acceleration component with different color\n",
    "#         ax.plot(subset_data['Time'], subset_data[accel_component], label=f'{accel_component} Acceleration', color=colors[j])\n",
    "#         ax.set_title(f\"Data for {unique_id} - {accel_component} Acceleration\")\n",
    "#         ax.set_xlabel('Time')\n",
    "#         ax.set_ylabel('Acceleration')\n",
    "#         ax.legend()\n",
    "#         ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxEDA.src.cvxEDA\n",
    "\n",
    "def calculate_eda_levels(y):\n",
    "    fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700}\n",
    "    Fs = fs_dict['EDA']\n",
    "    yn = (y - y.mean()) / y.std()\n",
    "    [r, p, t, l, d, e, obj] = cvxEDA.src.cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "    return [p, t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define unique_ids\n",
    "unique_ids = dataset['ID'].unique()\n",
    "\n",
    "# Iterate through each unique id\n",
    "for unique_id in unique_ids:\n",
    "    # Filter data for each id\n",
    "    subset_data = dataset[dataset['ID'] == unique_id]\n",
    "    \n",
    "    # Calculate EDA levels\n",
    "    tonic, phasic = calculate_eda_levels(subset_data['w_eda'].values)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.plot(tonic, label='Tonic')\n",
    "    plt.plot(phasic, label='Phasic')\n",
    "    plt.plot(subset_data['w_eda'].values, label='EDA')\n",
    "    \n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('EDA Levels')\n",
    "    plt.title(f'Phasic and Tonic EDA for ID: {unique_id}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before replacing labels\")\n",
    "unique_labels_before = sequences_df['downsampled_label'].unique()\n",
    "print(unique_labels_before, \"\\n\")\n",
    "print(\"Number of unique labels before replacement:\", len(unique_labels_before), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_df['downsampled_label'] = sequences_df['downsampled_label'].apply(lambda x : 1 if x == 2.0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "print(\"After replacing labels\")\n",
    "unique_labels_after = sequences_df['downsampled_label'].unique()\n",
    "print(unique_labels_after)\n",
    "print(\"Number of unique labels after replacement:\", len(unique_labels_after))\n",
    "\n",
    "le = preprocessing.LabelEncoder()  # Generates a look-up table\n",
    "le.fit(sequences_df['downsampled_label'])\n",
    "sequences_df['downsampled_label'] = le.transform(sequences_df['downsampled_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(sequences_df['downsampled_label'].unique())\n",
    "print(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def plot_label_distribution(df):\n",
    "    # Define class labels\n",
    "    sorts = {\n",
    "        0: \"No-stress\",\n",
    "        1: \"Stress\"\n",
    "    }\n",
    "\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(df['downsampled_label'])\n",
    "\n",
    "    # Extract counts for '0' and '1'\n",
    "    counts = [label_counts[0], label_counts[1]]\n",
    "    print(\"Label distribution:\", counts)\n",
    "\n",
    "    # Define bar labels\n",
    "    bar_labels = [sorts[0], sorts[1]]\n",
    "\n",
    "    # Plotting\n",
    "    plt.bar(bar_labels, counts)\n",
    "    plt.title(\"Number of samples per class\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_distribution(sequences_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the majority ('no-stress') and minority ('stress') classes\n",
    "df_no_stress = sequences_df[sequences_df['downsampled_label'] == 0]\n",
    "df_stress = sequences_df[sequences_df['downsampled_label'] == 1]\n",
    "\n",
    "# Downsample the majority class ('no-stress') to match the minority class ('stress')\n",
    "df_no_stress_downsampled = resample(df_no_stress,\n",
    "                                    replace=False,  # Sample without replacement\n",
    "                                    n_samples=len(df_stress),  # Match the number of 'stress' samples\n",
    "                                    random_state=42)  # Ensure reproducibility\n",
    "\n",
    "# Combine the downsampled 'no-stress' class with the 'stress' class\n",
    "sequences_df_balanced = pd.concat([df_no_stress_downsampled, df_stress])\n",
    "\n",
    "# Shuffle the combined dataset to mix the samples\n",
    "sequences_df_balanced = sequences_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Now sequences_df_balanced contains a balanced dataset with 'no-stress' samples evenly undersampled to match 'stress' samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_distribution(sequences_df_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Scale and split data****\n",
    "\n",
    "We perform a simple Min-Max scaling to bring the value-range between 0 and 1. We do not use Standard Scaling as the data does not follow a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the 'w_eda' feature\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "eda_series_list_scaled = [scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in sequences_df_balanced[\"w_eda\"]]\n",
    "\n",
    "# Convert the scaled feature back to a list of arrays\n",
    "eda_array_list = [np.array(series).flatten() for series in eda_series_list_scaled]\n",
    "\n",
    "# Separate the labels\n",
    "labels_list = [i for i in sequences_df_balanced['downsampled_label']]\n",
    "\n",
    "# Convert the labels list to numpy array\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "# print(len(combined_series_list))\n",
    "print(f\"EDA list Count:\", len(eda_series_list_scaled),\"\\n\" \"Labels list Count:\", len(labels_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Padding sequences to ensure uniform length\n",
    "max_sequence_length = 128  # Choose the desired maximum sequence length\n",
    "padded_series_list = pad_sequences(eda_series_list_scaled, maxlen=max_sequence_length, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "# Splitting data into training and testing sets (70% train, 30% test)\n",
    "x_temp, x_test, y_temp, y_test = train_test_split(\n",
    "    padded_series_list, labels_list, test_size=0.30, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Further splitting the training data into training and validation sets (80% train, 20% val from the original 70% train)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.20, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Convert to numpy arrays and reshape for compatibility with Keras\n",
    "x_train = np.asarray(x_train).astype(np.float32).reshape(-1, max_sequence_length, 1)  # Assuming 1 feature (EDA or TEMP)\n",
    "y_train = np.asarray(y_train).astype(np.float32).reshape(-1, 1)  # Do not one-hot encode\n",
    "\n",
    "x_val = np.asarray(x_val).astype(np.float32).reshape(-1, max_sequence_length, 1)  # Assuming 1 feature (EDA or TEMP)\n",
    "y_val = np.asarray(y_val).astype(np.float32).reshape(-1, 1)  # Do not one-hot encode\n",
    "\n",
    "x_test = np.asarray(x_test).astype(np.float32).reshape(-1, max_sequence_length, 1)  # Assuming 1 feature (EDA or TEMP)\n",
    "y_test = np.asarray(y_test).astype(np.float32).reshape(-1, 1)  # Do not one-hot encode\n",
    "\n",
    "# Create tf.data.Dataset objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)\n",
    "\n",
    "# Check lengths of train, validation, and test sets\n",
    "print(\n",
    "    f\"Length of x_train : {len(x_train)}\\nLength of x_val : {len(x_val)}\\nLength of x_test : {len(x_test)}\\n\"\n",
    "    f\"Length of y_train : {len(y_train)}\\nLength of y_val : {len(y_val)}\\nLength of y_test : {len(y_test)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_distribution(x_train, y_train, x_test, y_test, x_val, y_val):\n",
    "    \"\"\"\n",
    "    Plots a bar chart showing the sizes of the train, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train, y_train: Training data and labels.\n",
    "    - x_val, y_val: Validation data and labels.\n",
    "    - x_test, y_test: Test data and labels.\n",
    "    \"\"\"\n",
    "    dataset_names = ['Train', 'Test', 'Validation']\n",
    "    x_lengths = [len(x_train), len(x_test), len(x_val)]\n",
    "    y_lengths = [len(y_train), len(y_test), len(y_val)]\n",
    "    \n",
    "    # Plotting the bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.bar(dataset_names, x_lengths, color='b', alpha=0.6, label='X (Features)')\n",
    "    plt.bar(dataset_names, y_lengths, color='r', alpha=0.6, label='Y (Labels)', bottom=x_lengths)\n",
    "    \n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Dataset Distribution')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot dataset distribution\n",
    "plot_dataset_distribution(x_train, y_train, x_test, y_test, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tf.data.Datasets from numpy arrays\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# Shuffling and batching the datasets\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_dict = {}\n",
    "for i in sequences_df_balanced['downsampled_label']:\n",
    "    if i in vals_dict.keys():\n",
    "        vals_dict[i] += 1\n",
    "    else:\n",
    "        vals_dict[i] = 1\n",
    "total = sum(vals_dict.values())\n",
    "\n",
    "# Formula used - Naive method where\n",
    "# weight = 1 - (no. of samples present / total no. of samples)\n",
    "# So more the samples, lower the weight\n",
    "\n",
    "weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n",
    "print(weight_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your one-hot encoded labels are in a variable named 'labels'\n",
    "binary_labels = np.argmax(sequences_df_balanced['downsampled_label'])\n",
    "print(\"Shape of binary labels:\", binary_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_metrics(history: keras.callbacks.History):\n",
    "    total_plots = len(history.history)\n",
    "    cols = total_plots // 2\n",
    "\n",
    "    rows = total_plots // cols\n",
    "\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "\n",
    "    pos = range(1, total_plots + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (key, value) in enumerate(history.history.items()):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(range(len(value)), value)\n",
    "        plt.title(str(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_model():\n",
    "    input_layer = keras.Input(shape=(128, 1))\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=128, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=256, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=512, kernel_size=7, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=1024,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(4096, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Dense(2048, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = create_model()\n",
    "conv_model.summary()\n",
    "\n",
    "# Save model to JSON\n",
    "# Done for Kevin Bevers for his Headless CMS Stress platform prototype project\n",
    "model_json = conv_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"  # Change monitor to \"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",  # Change monitor to \"val_loss\"\n",
    "        factor=0.2,\n",
    "        patience=2,\n",
    "        min_lr=0.000001,\n",
    "    ),\n",
    "]\n",
    "\n",
    "optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=0.001)\n",
    "loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(),\n",
    "        keras.metrics.AUC(),\n",
    "        keras.metrics.Precision(),\n",
    "        keras.metrics.Recall(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "conv_model_history = conv_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_dataset,\n",
    "    class_weight=weight_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_metrics(conv_model_history)\n",
    "\n",
    "loss, accuracy, auc, precision, recall = conv_model.evaluate(train_dataset)\n",
    "print(f\"Loss : {loss}\")\n",
    "print(f\"Binary Accuracy : {accuracy}\")\n",
    "print(f\"Area under the Curve (ROC) : {auc}\")\n",
    "print(f\"Precision : {precision}\")\n",
    "print(f\"Recall : {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred_probs = conv_model.predict(x_test, verbose=0)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def view_evaluated_eeg_plots(model, sequences_df, scaler):\n",
    "    def plot_signals(data, labels, predictions, ids, times):\n",
    "        total_plots = len(data)\n",
    "        cols = total_plots // 5\n",
    "        rows = total_plots // cols\n",
    "        if total_plots % cols != 0:\n",
    "            rows += 1\n",
    "        pos = range(1, total_plots + 1)\n",
    "        fig = plt.figure(figsize=(40, 30))\n",
    "        for i, (plot_data, og_label, pred_label, id_, time) in enumerate(zip(data, labels, predictions, ids, times)):\n",
    "            plt.subplot(rows, cols, pos[i])\n",
    "            plt.plot(time, plot_data)\n",
    "            plt.title(f\"ID: {id_}\\nActual Label: {og_label}\\nPredicted Label: {pred_label}\")\n",
    "            fig.subplots_adjust(hspace=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    def generate_signals_for_label(label, num_signals=25):\n",
    "        filtered_df = sequences_df[sequences_df['downsampled_label'] == label]\n",
    "        sampled_df = filtered_df.sample(n=num_signals, random_state=42)\n",
    "        data = sampled_df['w_eda']\n",
    "        times = sampled_df['Time']\n",
    "        data_array = [scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in data]\n",
    "        data_array = np.asarray(data_array).astype(np.float32).reshape(-1, 128, 1)\n",
    "        labels = sampled_df['downsampled_label'].tolist()\n",
    "        ids = sampled_df['ID'].tolist()  # Extract IDs\n",
    "        predictions = (model.predict(data_array, verbose=0) > 0.5).astype(int).flatten()\n",
    "        return data, labels, predictions, ids, times\n",
    "\n",
    "    data_0, labels_0, predictions_0, ids_0, times_0 = generate_signals_for_label(0)\n",
    "    data_1, labels_1, predictions_1, ids_1, times_1 = generate_signals_for_label(1)\n",
    "    \n",
    "    print(\"Plotting signals with label 0:\")\n",
    "    plot_signals(data_0, labels_0, predictions_0, ids_0, times_0)\n",
    "    \n",
    "    print(\"Plotting signals with label 1:\")\n",
    "    plot_signals(data_1, labels_1, predictions_1, ids_1, times_1)\n",
    "\n",
    "# Call the function with the required arguments\n",
    "view_evaluated_eeg_plots(conv_model, sequences_df, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming sequences_df, scaler, and conv_model are already defined\n",
    "\n",
    "# Evaluate the model and print evaluation metrics\n",
    "loss, accuracy, auc, precision, recall = conv_model.evaluate(test_dataset)\n",
    "print(f\"Loss : {loss}\")\n",
    "print(f\"Top 3 Categorical Accuracy : {accuracy}\")\n",
    "print(f\"Area under the Curve (ROC) : {auc}\")\n",
    "print(f\"Precision : {precision}\")\n",
    "print(f\"Recall : {recall}\")\n",
    "\n",
    "def view_evaluated_eeg_plots(model):\n",
    "    start_index = random.randint(10, len(sequences_df) - 12)\n",
    "    end_index = start_index + 11\n",
    "    data = sequences_df.loc[start_index:end_index, 'w_eda']\n",
    "    time = sequences_df.loc[start_index:end_index, 'Time']\n",
    "    subjects = sequences_df.loc[start_index:end_index, 'ID']\n",
    "\n",
    "    # Ensure the time series data and time are aligned correctly\n",
    "    data_array = [scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in data]\n",
    "    data_array = np.asarray(data_array).astype(np.float32).reshape(-1, 128, 1)\n",
    "    \n",
    "    original_labels = sequences_df.loc[start_index:end_index, 'downsampled_label']\n",
    "    predicted_labels = np.argmax(model.predict(data_array, verbose=0), axis=1)\n",
    "\n",
    "    total_plots = len(data)\n",
    "    cols = total_plots // 3\n",
    "    rows = total_plots // cols\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "    pos = range(1, total_plots + 1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for i, (plot_data, og_label, pred_label, subject) in enumerate(zip(data, original_labels, predicted_labels, subjects)):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(time.iloc[i], plot_data)\n",
    "        plt.title(f\"Subject: {subject}\\nActual Label: {og_label}\\nPredicted Label: {pred_label}\")\n",
    "        fig.subplots_adjust(hspace=0.5)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "view_evaluated_eeg_plots(conv_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn -q\n",
    "%pip install pandas -q\n",
    "%pip install numpy -q\n",
    "%pip install matplotlib -q\n",
    "%pip install seaborn -q\n",
    "%pip install keras -q\n",
    "%pip install os -q\n",
    "\n",
    "%pip install cvxopt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing, model_selection\n",
    "import random\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cvxEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data\"\n",
    "\n",
    "QUALITY_THRESHOLD = 64\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "NUM_FOLDS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choice options of metrics are as follows:**\n",
    "- w_eda\n",
    "- cvx_phasic\n",
    "- cvx_tonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = \"w_eda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL CONFIGURATION**\n",
    "\n",
    "- adjust if necessary. This defines the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(DATA_PATH + \"/merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxEDA.src.cvxEDA\n",
    "\n",
    "def calculate_eda_levels(y):\n",
    "    fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700}\n",
    "    Fs = fs_dict['EDA']\n",
    "    yn = (y - y.mean()) / y.std()\n",
    "    [r, p, t, l, d, e, obj] = cvxEDA.src.cvxEDA.cvxEDA(yn, 1. / Fs)\n",
    "    return r, t, yn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define unique_ids\n",
    "unique_ids = dataset['ID'].unique()\n",
    "\n",
    "# Initialize the new DataFrame\n",
    "new_dataframe_eda = pd.DataFrame(columns=[\"cvx_phasic\", \"cvx_tonic\"])\n",
    "\n",
    "# Iterate through each unique id\n",
    "for unique_id in unique_ids:\n",
    "    # Filter data for each id\n",
    "    subset_data = dataset[dataset['ID'] == unique_id]\n",
    "    \n",
    "    # Calculate EDA levels\n",
    "    phasic, tonic, yn = calculate_eda_levels(subset_data['w_eda'].values)\n",
    "    \n",
    "    # Create a temporary DataFrame to hold the new data\n",
    "    temp_df = pd.DataFrame({\n",
    "        \"cvx_phasic\": phasic, \n",
    "        \"cvx_tonic\": tonic\n",
    "    })\n",
    "\n",
    "    new_dataframe_eda = pd.concat([new_dataframe_eda, temp_df], ignore_index=True)\n",
    "\n",
    "    # # Plotting\n",
    "    # plt.plot(tonic, label='Tonic')\n",
    "    # plt.plot(phasic, label='Phasic')\n",
    "    # plt.plot(subset_data['w_eda'].values, label='EDA')\n",
    "    \n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('EDA Levels')\n",
    "    # plt.title(f'Phasic and Tonic EDA for ID: {unique_id}')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, new_dataframe_eda], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to create sequences DataFrame\n",
    "def create_sequences_df(merged_df, max_length=32):\n",
    "    sequences = []\n",
    "    temp_sequence = []\n",
    "    eda_sequence = []\n",
    "    label = None\n",
    "    time_sequence = []\n",
    "    current_id = None\n",
    "    phasic_eda_sequence = []\n",
    "    tonic_eda_sequence = []\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        if current_id != row['ID']:\n",
    "            # New ID encountered, append previous sequence to list\n",
    "            if temp_sequence:\n",
    "                sequences.append({\n",
    "                    'ID': current_id,\n",
    "                    'w_eda': eda_sequence,\n",
    "                    'w_temp': temp_sequence,\n",
    "                    'downsampled_label': label,\n",
    "                    'Time': time_sequence,\n",
    "                    'cvx_phasic': phasic_eda_sequence,\n",
    "                    'cvx_tonic': tonic_eda_sequence\n",
    "                })\n",
    "            # Reset sequences for new ID\n",
    "            temp_sequence = [row['w_temp']]\n",
    "            eda_sequence = [row['w_eda']]\n",
    "            label = row['downsampled_labels']\n",
    "            time_sequence = [row['Time']]\n",
    "            current_id = row['ID']\n",
    "            phasic_eda_sequence = [row['cvx_phasic']]\n",
    "            tonic_eda_sequence = [row['cvx_tonic']]\n",
    "        else:\n",
    "            # Append values to sequences\n",
    "            temp_sequence.append(row['w_temp'])\n",
    "            eda_sequence.append(row['w_eda'])\n",
    "            time_sequence.append(row['Time'])\n",
    "            phasic_eda_sequence.append(row['cvx_phasic'])\n",
    "            tonic_eda_sequence.append(row['cvx_tonic'])\n",
    "\n",
    "        # Check if sequence length exceeds max_length\n",
    "        if len(temp_sequence) >= max_length:\n",
    "            sequences.append({\n",
    "                'ID': current_id,\n",
    "                'w_eda': eda_sequence,\n",
    "                'w_temp': temp_sequence,\n",
    "                'downsampled_label': label,\n",
    "                'Time': time_sequence,\n",
    "                'cvx_phasic': phasic_eda_sequence,\n",
    "                'cvx_tonic': tonic_eda_sequence\n",
    "            })\n",
    "            # Reset sequences for new ID\n",
    "            temp_sequence = []\n",
    "            eda_sequence = []\n",
    "            label = None\n",
    "            time_sequence = []\n",
    "            current_id = None\n",
    "            phasic_eda_sequence = []\n",
    "            tonic_eda_sequence = []\n",
    "\n",
    "    # Append last sequence if it's not empty\n",
    "    if temp_sequence:\n",
    "        sequences.append({\n",
    "            'ID': current_id,\n",
    "            'w_eda': eda_sequence,\n",
    "            'w_temp': temp_sequence,\n",
    "            'downsampled_label': label,\n",
    "            'Time': time_sequence,\n",
    "            'cvx_phasic': phasic_eda_sequence,\n",
    "            'cvx_tonic': tonic_eda_sequence\n",
    "        })\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    sequences_df = pd.DataFrame(sequences)\n",
    "    return sequences_df\n",
    "\n",
    "# Create sequences DataFrame\n",
    "sequences_df = create_sequences_df(dataset)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(sequences_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences_df.loc[88])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 8, figsize=(25, 6))  # Increased figure size\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define unique_ids\n",
    "unique_ids = dataset['ID'].unique()\n",
    "\n",
    "# Iterate through each unique id\n",
    "for i, unique_id in enumerate(unique_ids):\n",
    "    if i < len(unique_ids):\n",
    "        # Filter data for each id\n",
    "        subset_data = dataset[dataset['ID'] == unique_id]\n",
    "        \n",
    "        # Plotting\n",
    "        sns.lineplot(x='Time', y='w_eda', data=subset_data, ax=axes[i], color='blue', label='EDA')\n",
    "        sns.lineplot(x='Time', y='cvx_phasic', data=subset_data, ax=axes[i], color='red', label='phasic EDA')\n",
    "        # sns.lineplot(x='Time', y='cvx_tonic', data=subset_data, ax=axes[i], color='yellow', label='tonic EDA')                  \n",
    "        # sns.lineplot(x='Time', y='w_temp', data=subset_data, ax=axes[i], color='red', label='Temp')\n",
    "\n",
    "        axes[i].set_title(f\"Data for {unique_id}\")\n",
    "        axes[i].set_xlabel('Time')\n",
    "        axes[i].set_ylabel('Measurement')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create subplots\n",
    "# fig, axes = plt.subplots(16, 3, figsize=(60, 40))  # Increased figure size\n",
    "\n",
    "# # Define colors for each acceleration component\n",
    "# colors = ['red', 'green', 'blue']\n",
    "\n",
    "# # Iterate through each unique id\n",
    "# for i, unique_id in enumerate(unique_ids):\n",
    "#     # Filter data for each id\n",
    "#     subset_data = dataset[dataset['ID'] == unique_id]\n",
    "    \n",
    "#     # Iterate through X, Y, and Z accelerations\n",
    "#     for j, accel_component in enumerate(['X', 'Y', 'Z']):\n",
    "#         ax = axes[i, j]  # Select the appropriate subplot\n",
    "        \n",
    "#         # Plot acceleration component with different color\n",
    "#         ax.plot(subset_data['Time'], subset_data[accel_component], label=f'{accel_component} Acceleration', color=colors[j])\n",
    "#         ax.set_title(f\"Data for {unique_id} - {accel_component} Acceleration\")\n",
    "#         ax.set_xlabel('Time')\n",
    "#         ax.set_ylabel('Acceleration')\n",
    "#         ax.legend()\n",
    "#         ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# # Adjust layout\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before replacing labels\")\n",
    "unique_labels_before = sequences_df['downsampled_label'].unique()\n",
    "print(unique_labels_before, \"\\n\")\n",
    "print(\"Number of unique labels before replacement:\", len(unique_labels_before), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_df['downsampled_label'] = sequences_df['downsampled_label'].apply(lambda x : 1 if x == 2.0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "print(\"After replacing labels\")\n",
    "unique_labels_after = sequences_df['downsampled_label'].unique()\n",
    "print(unique_labels_after)\n",
    "print(\"Number of unique labels after replacement:\", len(unique_labels_after))\n",
    "\n",
    "le = preprocessing.LabelEncoder()  # Generates a look-up table\n",
    "le.fit(sequences_df['downsampled_label'])\n",
    "sequences_df['downsampled_label'] = le.transform(sequences_df['downsampled_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(sequences_df['downsampled_label'].unique())\n",
    "print(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def plot_label_distribution(df):\n",
    "    # Define class labels\n",
    "    sorts = {\n",
    "        0: \"No-stress\",\n",
    "        1: \"Stress\"\n",
    "    }\n",
    "\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(df['downsampled_label'])\n",
    "\n",
    "    # Extract counts for '0' and '1'\n",
    "    counts = [label_counts[0], label_counts[1]]\n",
    "    print(\"Label distribution:\", counts)\n",
    "\n",
    "    # Define bar labels\n",
    "    bar_labels = [sorts[0], sorts[1]]\n",
    "\n",
    "    # Plotting\n",
    "    plt.bar(bar_labels, counts)\n",
    "    plt.title(\"Number of samples per class\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_distribution(sequences_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.utils import resample\n",
    "\n",
    "# # Separate the majority ('no-stress') and minority ('stress') classes\n",
    "# df_no_stress = sequences_df[sequences_df['downsampled_label'] == 0]\n",
    "# df_stress = sequences_df[sequences_df['downsampled_label'] == 1]\n",
    "\n",
    "# # Downsample the majority class ('no-stress') to match the minority class ('stress')\n",
    "# df_no_stress_downsampled = resample(df_no_stress,\n",
    "#                                     replace=False,  # Sample without replacement\n",
    "#                                     n_samples=len(df_stress),  # Match the number of 'stress' samples\n",
    "#                                     random_state=42)  # Ensure reproducibility\n",
    "\n",
    "# # Combine the downsampled 'no-stress' class with the 'stress' class\n",
    "# sequences_df_balanced = pd.concat([df_no_stress_downsampled, df_stress])\n",
    "\n",
    "# # Shuffle the combined dataset to mix the samples\n",
    "# sequences_df_balanced = sequences_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences_df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_label_distribution(sequences_df_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Scale and split data****\n",
    "\n",
    "We perform a simple Min-Max scaling to bring the value-range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the 'w_eda' feature\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "eda_series_list = [scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in sequences_df[METRIC]]\n",
    "\n",
    "# Convert the scaled feature back to a list of arrays\n",
    "eda_array_list = [np.array(series).flatten() for series in eda_series_list]\n",
    "\n",
    "# Separate the labels\n",
    "labels_list = [i for i in sequences_df['downsampled_label']]\n",
    "\n",
    "# Convert the labels list to numpy array\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "# print(len(combined_series_list))\n",
    "print(f\"EDA list Count:\", len(eda_series_list),\"\\n\" \"Labels list Count:\", len(labels_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "max_sequence_length = 32  # Choose the desired maximum sequence length\n",
    "\n",
    "def apply_smote(x_train, y_train):\n",
    "    # Reshape input features if necessary\n",
    "    x_train_reshaped = x_train.reshape(x_train.shape[0], -1)\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    x_train_resampled, y_train_resampled = smote.fit_resample(x_train_reshaped, y_train)\n",
    "    \n",
    "    # Reshape resampled features back to original shape\n",
    "    x_train_resampled = x_train_resampled.reshape(-1, x_train.shape[1], x_train.shape[2])\n",
    "    \n",
    "    return x_train_resampled, y_train_resampled\n",
    "\n",
    "# Padding sequences to ensure uniform length\n",
    "padded_series_list = pad_sequences(eda_series_list, maxlen=max_sequence_length, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "# Splitting data into training and testing sets (70% train, 30% test)\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "    padded_series_list, labels_list, test_size=0.30, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Further splitting the training data into training and validation sets (80% train, 20% val from the original 70% train)\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "    x_train, y_train, test_size=0.20, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "x_train = np.asarray(x_train).astype(np.float32).reshape(-1, max_sequence_length, 1) \n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "\n",
    "x_val = np.asarray(x_val).astype(np.float32).reshape(-1, max_sequence_length, 1) \n",
    "y_val = np.asarray(y_val).astype(np.float32)\n",
    "\n",
    "x_test = np.asarray(x_test).astype(np.float32).reshape(-1, max_sequence_length, 1)\n",
    "y_test = np.asarray(y_test).astype(np.float32)\n",
    "\n",
    "# Check lengths of train, validation, and test sets\n",
    "print(\n",
    "    f\"Length of x_train : {len(x_train)}\\nLength of x_val : {len(x_val)}\\nLength of x_test : {len(x_test)}\\n\"\n",
    "    f\"Length of y_train : {len(y_train)}\\nLength of y_val : {len(y_val)}\\nLength of y_test : {len(y_test)}\"\n",
    ")\n",
    "\n",
    "# Check the class distribution before SMOTE\n",
    "print(\"Class distribution before SMOTE:\", Counter(y_train))\n",
    "\n",
    "# Apply SMOTE using the function\n",
    "x_train_resampled, y_train_resampled = apply_smote(x_train, y_train)\n",
    "\n",
    "# Check the class distribution after SMOTE\n",
    "class_distribution_after = Counter(y_train_resampled)\n",
    "print(\"Class distribution after SMOTE:\", {0: class_distribution_after[0], 1: class_distribution_after[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Class distribution before SMOTE\n",
    "class_distribution_before = Counter(y_train)\n",
    "# Class distribution after SMOTE\n",
    "class_distribution_after = Counter(y_train_resampled)\n",
    "\n",
    "# Define labels\n",
    "labels = ['No Stress', 'Stress']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot before SMOTE\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(labels, class_distribution_before.values(), color='blue')\n",
    "plt.title('Class Distribution Before SMOTE')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], labels)\n",
    "\n",
    "# Plot after SMOTE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(labels, class_distribution_after.values(), color='green')\n",
    "plt.title('Class Distribution After SMOTE')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_distribution(x_train, y_train, x_test, y_test, x_val, y_val):\n",
    "    \"\"\"\n",
    "    Plots a bar chart showing the sizes of the train, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - x_train, y_train: Training data and labels.\n",
    "    - x_val, y_val: Validation data and labels.\n",
    "    - x_test, y_test: Test data and labels.\n",
    "    \"\"\"\n",
    "    dataset_names = ['Train', 'Test', 'Validation']\n",
    "    x_lengths = [len(x_train), len(x_test), len(x_val)]\n",
    "    y_lengths = [len(y_train), len(y_test), len(y_val)]\n",
    "    \n",
    "    # Plotting the bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.bar(dataset_names, x_lengths, color='b', alpha=0.6, label='X (Features)')\n",
    "    plt.bar(dataset_names, y_lengths, color='r', alpha=0.6, label='Y (Labels)', bottom=x_lengths)\n",
    "    \n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Dataset Distribution')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot dataset distribution\n",
    "plot_dataset_distribution(x_train, y_train, x_test, y_test, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "def SplitDatasetForFolds(train_index, val_index, fold_nr):\n",
    "    print(f\"Training fold {fold_nr}...\")\n",
    "\n",
    "    # Split the data into train and validation sets for this fold.\n",
    "    x_train_fold = x_train[train_index]\n",
    "    y_train_fold = y_train[train_index]\n",
    "    x_val_fold = x_train[val_index]\n",
    "    y_val_fold = y_train[val_index]\n",
    "\n",
    "    # Create tf.data.Datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_fold, y_train_fold))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val_fold, y_val_fold))\n",
    "\n",
    "    # Shuffling and batching the datasets\n",
    "    train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    test_dataset = test_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    return train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_dict = {}\n",
    "for i in sequences_df['downsampled_label']:\n",
    "    if i in vals_dict.keys():\n",
    "        vals_dict[i] += 1\n",
    "    else:\n",
    "        vals_dict[i] = 1\n",
    "total = sum(vals_dict.values())\n",
    "\n",
    "# Formula used - Naive method where\n",
    "# weight = 1 - (no. of samples present / total no. of samples)\n",
    "# So more the samples, lower the weight\n",
    "\n",
    "weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n",
    "print(weight_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your one-hot encoded labels are in a variable named 'labels'\n",
    "binary_labels = np.argmax(sequences_df['downsampled_label'])\n",
    "print(\"Shape of binary labels:\", binary_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_metrics(history_dict: dict):\n",
    "    total_plots = len(history_dict)\n",
    "    cols = total_plots // 2\n",
    "    rows = total_plots // cols\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "\n",
    "    pos = range(1, total_plots + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (key, value) in enumerate(history_dict.items()):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(range(len(value)), value)\n",
    "        plt.title(str(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    input_layer = keras.Input(shape=(32, 1))\n",
    "    \n",
    "    x = layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = create_model()\n",
    "conv_model.summary()\n",
    "\n",
    "# Save model to JSON\n",
    "model_json = conv_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store history of each fold\n",
    "history_list = []\n",
    "fold_number = 1\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_model_filename = \"\"\n",
    "\n",
    "\n",
    "for train_index, val_index in kfold.split(x_train):\n",
    "    # Split data into training and validation sets for this fold.\n",
    "    train_dataset, test_dataset, val_dataset = SplitDatasetForFolds(train_index, val_index, fold_number)\n",
    "\n",
    "    # Create a new model instance\n",
    "    model = create_model()\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=0.001)\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(f\"best_model_fold_{fold_number}.keras\", save_best_only=True, monitor=\"val_binary_accuracy\"),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_binary_accuracy\", factor=0.2, patience=15, min_lr=0.000001),\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_binary_accuracy\", patience=10, restore_best_weights=True),\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=25,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=weight_dict\n",
    "    )\n",
    "\n",
    "    # Append history\n",
    "    history_list.append(history.history)\n",
    "\n",
    "        # Check if this model has the best validation accuracy so far\n",
    "    if history.history['val_binary_accuracy'][-1] > best_val_accuracy:\n",
    "        best_val_accuracy = history.history['val_binary_accuracy'][-1]\n",
    "        best_model_filename = f\"best_model_fold_{fold_number}.keras\"\n",
    "\n",
    "    fold_number += 1\n",
    "    print(f\"Training fold {fold_number} completed\\n\")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Cross-validation training completed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = keras.models.load_model(best_model_filename)\n",
    "\n",
    "# Now you have the best model, you can evaluate it on the test set\n",
    "for dataset in [train_dataset, val_dataset]:\n",
    "    print(\"Training set\" if dataset == train_dataset else \"Validation set\")\n",
    "    loss, binary_accuracy, auc, precision, recall = best_model.evaluate(dataset)\n",
    "    print(f\"Loss: {loss}\\n Binary Accuracy: {binary_accuracy}\\n AUC: {auc}\\n Precision: {precision}\\n Recall: {recall}\\n\")\n",
    "\n",
    "with open(f\"metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Loss: {loss}\\n Binary Accuracy: {binary_accuracy}\\n AUC: {auc}\\n Precision: {precision}\\n Recall: {recall}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a color scheme for metrics\n",
    "colors = ['b', 'g', 'r', 'y', 'k']\n",
    "\n",
    "# Plotting the metrics for all folds\n",
    "def plot_metrics(history_list, metrics, val_metrics, colors):\n",
    "    num_metrics = len(metrics)\n",
    "    fig, axs = plt.subplots(nrows=num_metrics, ncols=2, figsize=(28, 20))\n",
    "    for i, (metric, val_metric) in enumerate(zip(metrics, val_metrics)):\n",
    "        train_max = max([max(history[metric]) for history in history_list])\n",
    "        val_max = max([max(history[val_metric]) for history in history_list])\n",
    "        y_max = max(train_max, val_max)\n",
    "        for j, history in enumerate(history_list):\n",
    "            color_index = j % len(colors)  # Get color index for this fold\n",
    "            color = colors[color_index]     # Get color for this fold\n",
    "            axs[i, 0].plot(history[metric], label=f'Fold {j+1} {metric}', color=color)\n",
    "            axs[i, 1].plot(history[val_metric], label=f'Fold {j+1} {val_metric}', linestyle='--', color=color)\n",
    "        axs[i, 0].set_title(f'{metric.capitalize()} over Folds')\n",
    "        axs[i, 0].set_xlabel('Epochs')\n",
    "        axs[i, 0].set_ylabel(metric)\n",
    "        axs[i, 0].legend()\n",
    "        axs[i, 0].grid()\n",
    "        axs[i, 0].set_ylim([0, y_max])  # Set y-axis limit for training plot\n",
    "        \n",
    "        axs[i, 1].set_title(f'{val_metric.capitalize()} over Folds')\n",
    "        axs[i, 1].set_xlabel('Epochs')\n",
    "        axs[i, 1].set_ylabel(val_metric)\n",
    "        axs[i, 1].legend()\n",
    "        axs[i, 1].grid()\n",
    "        axs[i, 1].set_ylim([0, y_max])  # Set y-axis limit for validation plot\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Updated metrics list based on the actual keys from the history dictionary\n",
    "metrics = ['binary_accuracy', 'loss', 'auc', 'precision', 'recall']\n",
    "val_metrics = ['val_binary_accuracy', 'val_loss', 'val_auc', 'val_precision', 'val_recall']\n",
    "\n",
    "# Plot metrics\n",
    "plot_metrics(history_list, metrics, val_metrics, colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming best_model is already defined and trained\n",
    "# Generate predictions on the validation set\n",
    "y_pred_probs = best_model.predict(x_test, verbose=0)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Stress', 'Stress'], yticklabels=['No Stress', 'Stress'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Save the plot before showing it\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def view_evaluated_eeg_plots(model, sequences_df, scaler):\n",
    "    def plot_signals(data, labels, predictions, ids, times):\n",
    "        total_plots = len(data)\n",
    "        cols = total_plots // 5\n",
    "        rows = total_plots // cols\n",
    "        if total_plots % cols != 0:\n",
    "            rows += 1\n",
    "        pos = range(1, total_plots + 1)\n",
    "        fig = plt.figure(figsize=(40, 30))\n",
    "        for i, (plot_data, og_label, pred_label, id_, time) in enumerate(zip(data, labels, predictions, ids, times)):\n",
    "            plt.subplot(rows, cols, pos[i])\n",
    "            plt.plot(time, plot_data)\n",
    "            plt.title(f\"ID: {id_}\\nActual Label: {og_label}\\nPredicted Label: {pred_label}\")\n",
    "            fig.subplots_adjust(hspace=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    def generate_signals_for_label(label, num_signals=25):\n",
    "        filtered_df = sequences_df[sequences_df['downsampled_label'] == label]\n",
    "        sampled_df = filtered_df.sample(n=num_signals, random_state=42)\n",
    "        data = sampled_df['w_eda']\n",
    "        times = sampled_df['Time']\n",
    "        data_array = [scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in data]\n",
    "        data_array = np.asarray(data_array).astype(np.float32).reshape(-1, 32, 1)\n",
    "        labels = sampled_df['downsampled_label'].tolist()\n",
    "        ids = sampled_df['ID'].tolist()  # Extract IDs\n",
    "        predictions = (model.predict(data_array, verbose=0) > 0.5).astype(int).flatten()\n",
    "        return data, labels, predictions, ids, times\n",
    "\n",
    "    data_0, labels_0, predictions_0, ids_0, times_0 = generate_signals_for_label(0)\n",
    "    data_1, labels_1, predictions_1, ids_1, times_1 = generate_signals_for_label(1)\n",
    "    \n",
    "    print(\"Plotting signals with label 0:\")\n",
    "    plot_signals(data_0, labels_0, predictions_0, ids_0, times_0)\n",
    "    \n",
    "    print(\"Plotting signals with label 1:\")\n",
    "    plot_signals(data_1, labels_1, predictions_1, ids_1, times_1)\n",
    "\n",
    "# Call the function with the required arguments\n",
    "view_evaluated_eeg_plots(best_model, sequences_df, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def view_evaluated_eeg_plots(model, sequences_df, scaler, target_id):\n",
    "    def plot_signals(data, labels, predictions, ids, times):\n",
    "        total_plots = len(data)\n",
    "        cols = total_plots // 5\n",
    "        rows = total_plots // cols\n",
    "        if total_plots % cols != 0:\n",
    "            rows += 1\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*3))\n",
    "        for i, (plot_data, og_label, pred_label, id_, time) in enumerate(zip(data, labels, predictions, ids, times)):\n",
    "            if len(plot_data) == 0:  # Skip empty plots\n",
    "                continue\n",
    "            ax = axes[i // cols, i % cols] if rows > 1 else axes[i % cols]\n",
    "            color = 'green' if og_label == pred_label else 'red'\n",
    "            ax.plot(time, plot_data, color=color)\n",
    "            ax.set_title(f\"ID: {id_}\\nActual Label: {og_label}\\nPredicted Label: {pred_label}\")\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Data')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def generate_signals_for_id(target_id):\n",
    "        filtered_df = sequences_df[sequences_df['ID'] == target_id]\n",
    "        filtered_df = filtered_df.sort_values(by='Time')  # Sort by time\n",
    "        data = filtered_df['w_eda']\n",
    "        times = filtered_df['Time']\n",
    "        data_array = [scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in data]\n",
    "        data_array = np.asarray(data_array).astype(np.float32).reshape(-1, 32, 1)\n",
    "        labels = filtered_df['downsampled_label'].tolist()\n",
    "        ids = filtered_df['ID'].tolist()  # Extract IDs\n",
    "        predictions = (model.predict(data_array, verbose=0) > 0.5).astype(int).flatten()\n",
    "        return data, labels, predictions, ids, times\n",
    "\n",
    "    data, labels, predictions, ids, times = generate_signals_for_id(target_id)\n",
    "    \n",
    "    print(f\"Plotting signals for ID: {target_id}\")\n",
    "    plot_signals(data, labels, predictions, ids, times)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the required arguments\n",
    "view_evaluated_eeg_plots(best_model, sequences_df_balanced, scaler, target_id='S2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC, Precision, Recall, Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data/numpy\"\n",
    "MODEL_PATH = MAIN_PATH + \"/models\"\n",
    "LOG_PATH = MAIN_PATH + \"/logs\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "BEST_VAL_SCORE = 0\n",
    "BEST_MODEL = None\n",
    "HISTORY = []  # Initialize history_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_metrics(history_dict: dict):\n",
    "    total_plots = len(history_dict)\n",
    "    cols = total_plots // 2\n",
    "    rows = total_plots // cols\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "\n",
    "    pos = range(1, total_plots + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (key, value) in enumerate(history_dict.items()):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(range(len(value)), value)\n",
    "        plt.title(str(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df():\n",
    "    df = pd.read_csv(MAIN_PATH + \"/data/result_df.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_missing_values(numpy_data):\n",
    "    numpy_data['x_train'], numpy_data['y_train'] = Remove_missing_values(numpy_data['x_train'], numpy_data['y_train'])\n",
    "    numpy_data['x_val'], numpy_data['y_val'] = Remove_missing_values(numpy_data['x_val'], numpy_data['y_val'])\n",
    "    numpy_data['x_test_1'], numpy_data['y_test_1'] = Remove_missing_values(numpy_data['x_test_1'], numpy_data['y_test_1'])\n",
    "    numpy_data['x_test_2'], numpy_data['y_test_2'] = Remove_missing_values(numpy_data['x_test_2'], numpy_data['y_test_2'])\n",
    "    \n",
    "    return numpy_data\n",
    "\n",
    "def Remove_missing_values(x_data, y_data):\n",
    "    # Check if y_data contains missing values (NaNs) and remove corresponding x_data rows\n",
    "    valid_indices = ~np.isnan(y_data)  # Find valid (non-NaN) indices in y_data\n",
    "    x_clean = x_data[valid_indices]\n",
    "    y_clean = y_data[valid_indices]\n",
    "    return x_clean, y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_numpy_files(data_path):\n",
    "    numpy_data = {}\n",
    "    \n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith('.npy'):\n",
    "            file_path = os.path.join(data_path, file_name)\n",
    "            numpy_data[file_name[:-4]] = np.load(file_path)  # Store in dict\n",
    "    \n",
    "    # Clean data by removing rows where y_* contains missing values\n",
    "    numpy_data = Clean_missing_values(numpy_data)\n",
    "\n",
    "    return numpy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(df, label_column):\n",
    "    vals_dict = {}\n",
    "    for i in df[label_column]:\n",
    "        if i in vals_dict.keys():\n",
    "            vals_dict[i] += 1\n",
    "        else:\n",
    "            vals_dict[i] = 1\n",
    "    total = sum(vals_dict.values())\n",
    "    weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n",
    "\n",
    "    print(f\"Weight dict for model: {weight_dict}\")\n",
    "    return weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_layer = keras.Input(shape=(32, 1))\n",
    "    \n",
    "    x = layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\", kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compile_model():\n",
    "    model = create_model()\n",
    "    optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=0.001)\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            F1Score(name='f1_score')\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDatasetForFolds(train_index, validation_index, fold_nr, numpy_data):\n",
    "    print(f\"Training fold {fold_nr}...\")\n",
    "\n",
    "    # Split the data into train sets for this fold.\n",
    "    x_train_fold = numpy_data['x_train'][train_index]\n",
    "    y_train_fold = numpy_data['y_train'][train_index]\n",
    "\n",
    "    print(f\"x_val shape: {numpy_data['x_val'].shape}\")\n",
    "    print(f\"y_val shape: {numpy_data['y_val'].shape}\")\n",
    "    \n",
    "    # Ensure to use only the training set indices\n",
    "    x_validation_fold = numpy_data['x_val'][:len(validation_index)]  # Taking the first `len(validation_index)` samples\n",
    "    y_validation_fold = numpy_data['y_val'][:len(validation_index)]\n",
    "\n",
    "    # Create tf.data.Datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_fold, y_train_fold))\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((x_validation_fold, y_validation_fold))\n",
    "    test_dataset_subject1 = tf.data.Dataset.from_tensor_slices((numpy_data['x_test_1'], numpy_data['y_test_1']))\t\n",
    "    test_dataset_subject2 = tf.data.Dataset.from_tensor_slices((numpy_data['x_test_2'], numpy_data['y_test_2']))\n",
    "    \n",
    "    # Shuffling and batching the datasets\n",
    "    train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    validation_dataset = validation_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    test_dataset_subject1 = test_dataset_subject1.batch(BATCH_SIZE)\n",
    "    test_dataset_subject2 = test_dataset_subject2.batch(BATCH_SIZE)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset_subject1, test_dataset_subject2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_native(data):\n",
    "    \"\"\"Recursively convert numpy types to native python types.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_to_native(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_to_native(item) for item in data]\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return data.tolist()  # Convert numpy array to list\n",
    "    elif isinstance(data, (np.float32, np.float64)):\n",
    "        return data.item()  # Convert single value numpy float to Python float\n",
    "    else:\n",
    "        return data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_to_json(history, fold_number, best_model):\n",
    "    \"\"\"Save the training history and best model path to a JSON file.\"\"\"\n",
    "    try:\n",
    "        history_data = {\n",
    "            \"history\": convert_to_native(history.history),\n",
    "            \"best_model\": best_model\n",
    "        }\n",
    "        \n",
    "        history_file_path = os.path.join(LOG_PATH, f\"history_fold_{fold_number}.json\")\n",
    "        with open(history_file_path, 'w') as json_file:\n",
    "            json.dump(history_data, json_file)  # Write the history and best model to a JSON file\n",
    "        print(f\"History and best model saved to {history_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving history: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_fold(train_index, val_index, fold_number, numpy_data, weight_dict):\n",
    "    global BEST_VAL_SCORE, BEST_MODEL, HISTORY # Use global variables\n",
    "\n",
    "    # Split data into training and validation sets for this fold.\n",
    "    train_dataset, validation_dataset, test_sj1, test_sj2 = SplitDatasetForFolds(train_index, val_index, fold_number, numpy_data)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Compile_model()\n",
    "\n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(os.path.join(MODEL_PATH, f\"best_model_fold_{fold_number}.keras\"), save_best_only=True, monitor=\"val_binary_accuracy\"),\n",
    "        # keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=15,\n",
    "        validation_data=(numpy_data['x_val'], numpy_data['y_val']),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=weight_dict\n",
    "    )\n",
    "\n",
    "    # Append history\n",
    "    HISTORY.append(history.history)\n",
    "\n",
    "    # Save the history to a JSON file\n",
    "    save_history_to_json(history, fold_number, BEST_MODEL)\n",
    "\n",
    "    # Check if this model has the best validation accuracy so far\n",
    "    if history.history['val_f1_score'][-1] > BEST_VAL_SCORE:\n",
    "        BEST_VAL_SCORE = history.history['val_f1_score'][-1]\n",
    "        BEST_MODEL = os.path.join(MODEL_PATH, f\"best_model_fold_{fold_number}.keras\")\n",
    "\n",
    "    print(f\"Training fold {fold_number} completed\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_validation_training(numpy_data, weight_dict):\n",
    "    global fold_number\n",
    "    fold_number = 1\n",
    "    \n",
    "    # Initialize KFold with the number of splits\n",
    "    kfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "    try:\n",
    "        for train_index, val_index in kfold.split(numpy_data['x_train']):         \n",
    "            print(f\"Train indices: {train_index}\")\n",
    "            print(f\"Validation indices: {val_index}\")\n",
    "            print(f\"Max validation index: {max(val_index)}\")\n",
    "            \n",
    "            # Train fold\n",
    "            Train_fold(train_index, val_index, fold_number, numpy_data, weight_dict)\n",
    "            fold_number += 1\n",
    "\n",
    "        print(\"Cross-validation training completed.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during cross-validation training: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = load_df();\n",
    "    numpy_data = gather_numpy_files(DATA_PATH)\n",
    "\n",
    "    # Calculate weights\n",
    "    weight_dict = calculate_class_weights(df, 'downsampled_label')\n",
    "\n",
    "    # Create model\n",
    "    convolutional_model = create_model()\n",
    "    convolutional_model.summary()\n",
    "\n",
    "    # Train model\n",
    "    Cross_validation_training(numpy_data, weight_dict)\n",
    "\n",
    "\n",
    "    return numpy_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

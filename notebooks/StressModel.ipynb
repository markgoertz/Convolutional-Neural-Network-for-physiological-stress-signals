{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.callbacks\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC, Precision, Recall, Metric\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model  # Import Model here\n",
    "\n",
    "\n",
    "from dvclive import Live  # Ensure DVCLive is imported\n",
    "from dvclive.keras import DVCLiveCallback  # Import the callback\n",
    "import yaml\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data/results\"\n",
    "MODEL_PATH = MAIN_PATH + \"/models\"\n",
    "LOG_PATH = MAIN_PATH + \"/logs\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "BEST_VAL_SCORE = 0\n",
    "BEST_MODEL = None\n",
    "HISTORY = []  # Initialize history_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path=MAIN_PATH + \"/params.yaml\"):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config()  # This loads the configuration once and allows direct access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_metrics(history_dict: dict):\n",
    "    total_plots = len(history_dict)\n",
    "    cols = total_plots // 2\n",
    "    rows = total_plots // cols\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "\n",
    "    pos = range(1, total_plots + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (key, value) in enumerate(history_dict.items()):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(range(len(value)), value)\n",
    "        plt.title(str(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df():\n",
    "    df = pd.read_csv(MAIN_PATH + \"/data/result_df.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_missing_values(numpy_data):\n",
    "    numpy_data['x_train'], numpy_data['y_train'] = Remove_missing_values(numpy_data['x_train'], numpy_data['y_train'])\n",
    "    numpy_data['x_val'], numpy_data['y_val'] = Remove_missing_values(numpy_data['x_val'], numpy_data['y_val'])\n",
    "    numpy_data['x_test_1'], numpy_data['y_test_1'] = Remove_missing_values(numpy_data['x_test_1'], numpy_data['y_test_1'])\n",
    "    numpy_data['x_test_2'], numpy_data['y_test_2'] = Remove_missing_values(numpy_data['x_test_2'], numpy_data['y_test_2'])\n",
    "    \n",
    "    return numpy_data\n",
    "\n",
    "def Remove_missing_values(x_data, y_data):\n",
    "    # Check if y_data contains missing values (NaNs) and remove corresponding x_data rows\n",
    "    valid_indices = ~np.isnan(y_data)  # Find valid (non-NaN) indices in y_data\n",
    "    x_clean = x_data[valid_indices]\n",
    "    y_clean = y_data[valid_indices]\n",
    "    return x_clean, y_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_pickles_and_convert_to_numpy_with_columns(directory):\n",
    "    try:\n",
    "        # Dictionary to store the loaded data as NumPy arrays\n",
    "        numpy_data = {}\n",
    "        \n",
    "        # List all files in the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".pkl\"):  # Only process .pkl files\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                \n",
    "                # Load the data from the .pkl file\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    data = pickle.load(file)\n",
    "                \n",
    "                # Store the data in the dictionary, using the file name (without .pkl) as the key\n",
    "                dataset_name = filename.replace(\".pkl\", \"\")\n",
    "                \n",
    "                # Print to verify if DataFrame still has columns (for debugging)\n",
    "                if isinstance(data, pd.DataFrame):\n",
    "                    # Convert the DataFrame into a dictionary of NumPy arrays, one for each column\n",
    "                    numpy_data[dataset_name] = {col: np.array(data[col].tolist()) for col in data.columns}\n",
    "                else:\n",
    "                    # If the dataset is not a DataFrame (like labels), convert directly to NumPy array\n",
    "                    numpy_data[dataset_name] = np.array(data)\n",
    "                \n",
    "                print(f\"Loaded {filename} successfully.\")\n",
    "        \n",
    "        return numpy_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load and convert datasets to NumPy arrays: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(df, label_column):\n",
    "    vals_dict = {}\n",
    "    for i in df[label_column]:\n",
    "        if i in vals_dict.keys():\n",
    "            vals_dict[i] += 1\n",
    "        else:\n",
    "            vals_dict[i] = 1\n",
    "    total = sum(vals_dict.values())\n",
    "    weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n",
    "\n",
    "    print(f\"Weight dict for model: {weight_dict}\")\n",
    "    return weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class F1Score(Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_head():\n",
    "    input_layer = tf.keras.Input(shape=(config[\"model\"][\"input_shape\"], config[\"model\"][\"input_features\"]))\n",
    "    # First convolutional layer\n",
    "    x = tf.keras.layers.Conv1D(filters=32, kernel_size=config[\"model\"][\"kernel_size\"], \n",
    "                      activation=config[\"model\"][\"activation\"], padding=\"same\", \n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001))(input_layer)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    x = tf.keras.layers.Conv1D(64, kernel_size=config[\"model\"][\"kernel_size\"], activation=config[\"model\"][\"activation\"])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(filters=128, kernel_size=config[\"model\"][\"kernel_size\"], \n",
    "                      activation=config[\"model\"][\"activation\"], padding=\"same\", \n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Flatten the output\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_layers, model_heads):\n",
    "    # Merge models using their outputs directly\n",
    "    combined = tf.keras.layers.concatenate(model_heads)\n",
    "    \n",
    "    # Add additional layers after merging\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(combined)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # Adjust based on your task\n",
    "\n",
    "    # Final model\n",
    "    model = keras.Model(inputs=input_layers, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Adjust loss if necessary\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_val, y_val, class_weight):\n",
    "    \"\"\"Trains the model on the training data.\"\"\"\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=config['model']['epochs'],  # Adjust epochs\n",
    "        batch_size=config['model']['batch_size'],  # Adjust batch size\n",
    "        class_weight=class_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model():\n",
    "    model_head = create_model_head()\n",
    "    input_layer = Input(shape=(config[\"model\"][\"input_shape\"], config[\"model\"][\"input_features\"]))\n",
    "    model = Model(inputs=input_layer, outputs=model_head)\n",
    "    optimizer = Adam(amsgrad=True, learning_rate=config[\"model\"][\"learning_rate\"])\n",
    "    loss = BinaryCrossentropy()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[\n",
    "            BinaryAccuracy(name='binary_accuracy'),\n",
    "            AUC(name='auc'),\n",
    "            Precision(name='precision'),\n",
    "            Recall(name='recall'),\n",
    "            F1Score(name='f1_score')\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDatasetForFolds(train_index, validation_index, fold_nr, numpy_data,metric,labels='labels'):\n",
    "    print(f\"Training fold {fold_nr+1} + metric {metric}\")\n",
    "\n",
    "    # Split the data into train sets for this fold.\n",
    "    x_train_fold = numpy_data['x_train'][metric][train_index]\n",
    "    y_train_fold = numpy_data['y_train'][labels][train_index]\n",
    "    \n",
    "    x_validation_fold = numpy_data['x_val'][metric][:len(validation_index)] \n",
    "    y_validation_fold = numpy_data['y_val'][labels][:len(validation_index)]\n",
    "\n",
    "    x_train_fold = x_train_fold.reshape(-1, 32, 1)\n",
    "    x_validation_fold = x_validation_fold.reshape(-1, 32, 1)\n",
    "    y_train_fold = y_train_fold.reshape(-1, 1)\n",
    "    y_validation_fold = y_validation_fold.reshape(-1, 1)\n",
    "\n",
    "    print(f'x_train_fold shape: {x_train_fold.shape}')\n",
    "    print(f'y_train_fold shape: {y_train_fold.shape}')\n",
    "    print(f'x_validation_fold shape: {x_validation_fold.shape}')\n",
    "    print(f'y_validation_fold shape: {y_validation_fold.shape}')\n",
    "    \n",
    "    # Create tf.data.Datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_fold, y_train_fold))\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((x_validation_fold, y_validation_fold))\n",
    "    test_dataset_subject1 = tf.data.Dataset.from_tensor_slices((numpy_data['x_test_1'][metric], numpy_data['y_test_1'][labels]))    \n",
    "    test_dataset_subject2 = tf.data.Dataset.from_tensor_slices((numpy_data['x_test_2'][metric], numpy_data['y_test_2'][labels]))\n",
    "    \n",
    "    # Shuffling and batching the datasets\n",
    "    train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    validation_dataset = validation_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    test_dataset_subject1 = test_dataset_subject1.batch(BATCH_SIZE)\n",
    "    test_dataset_subject2 = test_dataset_subject2.batch(BATCH_SIZE)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset_subject1, test_dataset_subject2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_to_json(history, fold_number, best_model):\n",
    "    # Create a dictionary for the current fold's metrics\n",
    "    metrics = {\n",
    "        \"fold_number\": fold_number,\n",
    "        \"val_accuracy\": history.history['val_accuracy'][-1],\n",
    "        \"val_loss\": history.history['val_loss'][-1],\n",
    "        \"best_model\": best_model\n",
    "    }\n",
    "\n",
    "    # Load existing metrics if the file exists\n",
    "    if os.path.exists('metrics.json'):\n",
    "        with open('metrics.json', 'r') as f:\n",
    "            existing_metrics = json.load(f)\n",
    "    else:\n",
    "        existing_metrics = []\n",
    "\n",
    "    # Append the new metrics\n",
    "    existing_metrics.append(metrics)\n",
    "\n",
    "    # Write back the updated metrics to the file\n",
    "    with open('metrics.json', 'w') as f:\n",
    "        json.dump(existing_metrics, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_fold(train_index, val_index, fold_number, numpy_data, weight_dict):\n",
    "    exp_mess = f\"fold-{fold_number}\".lower()\n",
    "    print(f\"Experiment name: {exp_mess}\")\n",
    "    \n",
    "    with Live(exp_message=f\"Training fold {exp_mess}\") as live:\n",
    "        # Split data into training and validation sets for this fold\n",
    "        train_dataset, validation_dataset, test_dataset_subject1, test_dataset_subject2 = SplitDatasetForFolds(train_index, val_index, fold_number, numpy_data)\n",
    "\n",
    "       # Create and compile the model\n",
    "        model = Compile_model()   \n",
    "        # Set up callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                os.path.join(MODEL_PATH, f\"best_model_fold_{fold_number}.keras\"),\n",
    "                save_best_only=True,\n",
    "                monitor=\"val_accuracy\"\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "            DVCLiveCallback()\n",
    "        ]\n",
    "\n",
    "        # Log parameters for this fold\n",
    "        live.log_param(\"fold_number\", fold_number)\n",
    "        live.log_param(\"epochs\", config['model']['epochs'])\n",
    "        live.log_param(\"batch_size\", config['model']['batch_size'])\n",
    "        live.log_param(\"learning_rate\", config['model']['learning_rate'])\n",
    "        live.log_param(\"folds\", config['model']['folds'])\n",
    "        live.log_param(\"kernel_size\", config['model']['kernel_size'])\n",
    "        live.log_param(\"activation\", config['model']['activation'])\n",
    "        live.log_param(\"input_shape\", config['model']['input_shape'])\n",
    "        live.log_param(\"input_features\", config['model']['input_features'])\n",
    "        live.log_param(\"shuffle_buffer_size\", config['model']['shuffle_buffer_size'])\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            epochs=config['model']['epochs'],\n",
    "            validation_data=(validation_dataset),\n",
    "            callbacks=callbacks,\n",
    "            class_weight=weight_dict\n",
    "        )\n",
    "\n",
    "        # Save the model to the models directory\n",
    "        os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "        model.save(os.path.join(MODEL_PATH, f'model_{fold_number}.h5'))\n",
    "        print(f'Model saved to {MODEL_PATH}/model_{fold_number}.h5')\n",
    "\n",
    "        print(f\"Training fold {fold_number} completed\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_validation_training(data, weight_dict):\n",
    "    scores = []\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)  # Ensure the model path exists\n",
    "\n",
    "    metric = config['model']['metrics'][0]\n",
    "    # Flatten x_train to (n_samples, n_timesteps * n_features)\n",
    "   \n",
    "    kfold = KFold(n_splits=config['model']['folds'], shuffle=True, random_state=42)\n",
    "    x_train = (data['x_train'][metric])\n",
    "    try:\n",
    "        for fold_number, (train_index, val_index) in enumerate(kfold.split(x_train)):\n",
    "            train_dataset, validation_dataset, test_dataset_subject1, test_dataset_subject2 = SplitDatasetForFolds(\n",
    "                train_index, val_index, fold_number, data, metric)           \n",
    "            model = compile_model()\n",
    "\n",
    "            # Train model\n",
    "            train_model(model, train_dataset, validation_dataset, weight_dict)\n",
    "            print(f'training: {train_dataset}')\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during cross-validation training: {e}\")\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(data, metrics):\n",
    "    filtered_data = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict) and key.startswith('x_'):\n",
    "            filtered_data[key] = {k: v for k, v in value.items() if k in metrics}\n",
    "        else:\n",
    "            filtered_data[key] = value\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = load_df()\n",
    "    datasets = load_all_pickles_and_convert_to_numpy_with_columns(DATA_PATH)\n",
    "    \n",
    "    print(f\"Loaded datasets: {datasets.keys()}\")\n",
    "\n",
    "    # Calculate weights\n",
    "    weight_dict = calculate_class_weights(df, 'downsampled_label')\n",
    "    \n",
    "    # Filter columns based on config['model']['metrics']\n",
    "    datasets = filter_columns(datasets, config['model']['metrics'])\n",
    "    for key, value in datasets.items():\n",
    "        if isinstance(value, dict):\n",
    "            for sub_key, sub_value in value.items():\n",
    "                print(f\"{key} - {sub_key}: {sub_value.shape + (1,)}\")\n",
    "\n",
    "    # return datasets\n",
    "    # Train model\n",
    "    x = Cross_validation_training(datasets, weight_dict)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded x_test_1.pkl successfully.\n",
      "Loaded x_test_2.pkl successfully.\n",
      "Loaded x_train.pkl successfully.\n",
      "Loaded x_val.pkl successfully.\n",
      "Loaded y_test_1.pkl successfully.\n",
      "Loaded y_test_2.pkl successfully.\n",
      "Loaded y_train.pkl successfully.\n",
      "Loaded y_val.pkl successfully.\n",
      "Loaded datasets: dict_keys(['x_test_1', 'x_test_2', 'x_train', 'x_val', 'y_test_1', 'y_test_2', 'y_train', 'y_val'])\n",
      "Weight dict for model: {0: 0.11450662739322537, 1: 0.8854933726067746}\n",
      "x_test_1 - EDA: (704, 32, 1)\n",
      "x_test_1 - TEMP: (704, 32, 1)\n",
      "x_test_1 - BVP: (704, 32, 1)\n",
      "x_test_2 - EDA: (740, 32, 1)\n",
      "x_test_2 - TEMP: (740, 32, 1)\n",
      "x_test_2 - BVP: (740, 32, 1)\n",
      "x_train - EDA: (11704, 32, 1)\n",
      "x_train - TEMP: (11704, 32, 1)\n",
      "x_train - BVP: (11704, 32, 1)\n",
      "x_val - EDA: (4998, 32, 1)\n",
      "x_val - TEMP: (4998, 32, 1)\n",
      "x_val - BVP: (4998, 32, 1)\n",
      "y_test_1 - labels: (704, 1)\n",
      "y_test_2 - labels: (740, 1)\n",
      "y_train - labels: (11704, 1)\n",
      "y_val - labels: (4998, 1)\n",
      "Training fold 1 + metric EDA\n",
      "x_train_fold shape: (9363, 32, 1)\n",
      "y_train_fold shape: (9363, 1)\n",
      "x_validation_fold shape: (2341, 32, 1)\n",
      "y_validation_fold shape: (2341, 1)\n",
      "An error occurred during cross-validation training: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 32, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"conv1d\". The following previous layers were accessed without issue: []\n"
     ]
    }
   ],
   "source": [
    "x = main()\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC, Precision, Recall, Metric\n",
    "\n",
    "from dvclive import Live  # Ensure DVCLive is imported\n",
    "from dvclive.keras import DVCLiveCallback  # Import the callback\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data/numpy\"\n",
    "MODEL_PATH = MAIN_PATH + \"/models\"\n",
    "LOG_PATH = MAIN_PATH + \"/logs\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "BEST_VAL_SCORE = 0\n",
    "BEST_MODEL = None\n",
    "HISTORY = []  # Initialize history_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path=MAIN_PATH + \"/params.yaml\"):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config()  # This loads the configuration once and allows direct access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_metrics(history_dict: dict):\n",
    "    total_plots = len(history_dict)\n",
    "    cols = total_plots // 2\n",
    "    rows = total_plots // cols\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "\n",
    "    pos = range(1, total_plots + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (key, value) in enumerate(history_dict.items()):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(range(len(value)), value)\n",
    "        plt.title(str(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df():\n",
    "    df = pd.read_csv(MAIN_PATH + \"/data/result_df.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_missing_values(numpy_data):\n",
    "    numpy_data['x_train'], numpy_data['y_train'] = Remove_missing_values(numpy_data['x_train'], numpy_data['y_train'])\n",
    "    numpy_data['x_val'], numpy_data['y_val'] = Remove_missing_values(numpy_data['x_val'], numpy_data['y_val'])\n",
    "    numpy_data['x_test_1'], numpy_data['y_test_1'] = Remove_missing_values(numpy_data['x_test_1'], numpy_data['y_test_1'])\n",
    "    numpy_data['x_test_2'], numpy_data['y_test_2'] = Remove_missing_values(numpy_data['x_test_2'], numpy_data['y_test_2'])\n",
    "    \n",
    "    return numpy_data\n",
    "\n",
    "def Remove_missing_values(x_data, y_data):\n",
    "    # Check if y_data contains missing values (NaNs) and remove corresponding x_data rows\n",
    "    valid_indices = ~np.isnan(y_data)  # Find valid (non-NaN) indices in y_data\n",
    "    x_clean = x_data[valid_indices]\n",
    "    y_clean = y_data[valid_indices]\n",
    "    return x_clean, y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_numpy_files(data_path):\n",
    "    numpy_data = {}\n",
    "    \n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith('.npy'):\n",
    "            file_path = os.path.join(data_path, file_name)\n",
    "            numpy_data[file_name[:-4]] = np.load(file_path)  # Store in dict\n",
    "    \n",
    "    # Clean data by removing rows where y_* contains missing values\n",
    "    numpy_data = Clean_missing_values(numpy_data)\n",
    "\n",
    "    return numpy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(df, label_column):\n",
    "    vals_dict = {}\n",
    "    for i in df[label_column]:\n",
    "        if i in vals_dict.keys():\n",
    "            vals_dict[i] += 1\n",
    "        else:\n",
    "            vals_dict[i] = 1\n",
    "    total = sum(vals_dict.values())\n",
    "    weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n",
    "\n",
    "    print(f\"Weight dict for model: {weight_dict}\")\n",
    "    return weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_layer = keras.Input(shape=(config[\"model\"][\"input_shape\"], config[\"model\"][\"input_features\"]))\n",
    "    \n",
    "    x = layers.Conv1D(filters=32, kernel_size=config[\"model\"][\"kernel_size\"], activation=config[\"model\"][\"activation\"], padding=\"same\", kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, activation=config[\"model\"][\"activation\"], padding=\"same\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, activation=config[\"model\"][\"activation\"], padding=\"same\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compile_model():\n",
    "    model = create_model()\n",
    "    optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=config[\"model\"][\"learning_rate\"])\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            F1Score(name='f1_score')\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDatasetForFolds(train_index, validation_index, fold_nr, numpy_data):\n",
    "    print(f\"Training fold {fold_nr}...\")\n",
    "\n",
    "    # Split the data into train sets for this fold.\n",
    "    x_train_fold = numpy_data['x_train'][train_index]\n",
    "    y_train_fold = numpy_data['y_train'][train_index]\n",
    "\n",
    "    print(f\"x_val shape: {numpy_data['x_val'].shape}\")\n",
    "    print(f\"y_val shape: {numpy_data['y_val'].shape}\")\n",
    "    \n",
    "    # Ensure to use only the training set indices\n",
    "    x_validation_fold = numpy_data['x_val'][:len(validation_index)]  # Taking the first `len(validation_index)` samples\n",
    "    y_validation_fold = numpy_data['y_val'][:len(validation_index)]\n",
    "\n",
    "    # Create tf.data.Datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_fold, y_train_fold))\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((x_validation_fold, y_validation_fold))\n",
    "    test_dataset_subject1 = tf.data.Dataset.from_tensor_slices((numpy_data['x_test_1'], numpy_data['y_test_1']))\t\n",
    "    test_dataset_subject2 = tf.data.Dataset.from_tensor_slices((numpy_data['x_test_2'], numpy_data['y_test_2']))\n",
    "    \n",
    "    # Shuffling and batching the datasets\n",
    "    train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    validation_dataset = validation_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    test_dataset_subject1 = test_dataset_subject1.batch(BATCH_SIZE)\n",
    "    test_dataset_subject2 = test_dataset_subject2.batch(BATCH_SIZE)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset_subject1, test_dataset_subject2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_native(data):\n",
    "    \"\"\"Recursively convert numpy types to native python types.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_to_native(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_to_native(item) for item in data]\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return data.tolist()  # Convert numpy array to list\n",
    "    elif isinstance(data, (np.float32, np.float64)):\n",
    "        return data.item()  # Convert single value numpy float to Python float\n",
    "    else:\n",
    "        return data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_to_json(history, fold_number, best_model):\n",
    "    # Create a dictionary for the current fold's metrics\n",
    "    metrics = {\n",
    "        \"fold_number\": fold_number,\n",
    "        \"val_accuracy\": history.history['val_accuracy'][-1],\n",
    "        \"val_loss\": history.history['val_loss'][-1],\n",
    "        \"best_model\": best_model\n",
    "    }\n",
    "\n",
    "    # Load existing metrics if the file exists\n",
    "    if os.path.exists('metrics.json'):\n",
    "        with open('metrics.json', 'r') as f:\n",
    "            existing_metrics = json.load(f)\n",
    "    else:\n",
    "        existing_metrics = []\n",
    "\n",
    "    # Append the new metrics\n",
    "    existing_metrics.append(metrics)\n",
    "\n",
    "    # Write back the updated metrics to the file\n",
    "    with open('metrics.json', 'w') as f:\n",
    "        json.dump(existing_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_fold(train_index, val_index, fold_number, numpy_data, weight_dict, live):\n",
    "    global BEST_VAL_SCORE, HISTORY \n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    train_dataset, validation_dataset, test_sj1, test_sj2 = SplitDatasetForFolds(train_index, val_index, fold_number, numpy_data)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Compile_model()\n",
    "\n",
    "    # Set up DVC Live callback for this fold\n",
    "    live_callback = DVCLiveCallback()  # No need to pass live here\n",
    "\n",
    "    # Set up other callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODEL_PATH, f\"best_model_fold_{fold_number}.keras\"),\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_accuracy\"\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "        live_callback  # Add DVCLiveCallback to the list\n",
    "    ]\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=config['model']['epochs'],\n",
    "        validation_data=(validation_dataset),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=weight_dict\n",
    "    )\n",
    "\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    \n",
    "    # Save the model to the models directory\n",
    "    model.save(os.path.join(MODEL_PATH, f'model_{fold_number}.h5'))\n",
    "    print(f'Model saved to models/model_{fold_number}.h5')\n",
    "\n",
    "    print(f\"Training fold {fold_number} completed\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_validation_training(numpy_data, weight_dict):\n",
    "    global fold_number\n",
    "    fold_number = 1\n",
    "\n",
    "    # Initialize KFold with the number of splits\n",
    "    kfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    try:\n",
    "        for train_index, val_index in kfold.split(numpy_data['x_train']):\n",
    "            print(f\"Training fold {fold_number}\")\n",
    "\n",
    "            # Initialize DVCLive for the current fold\n",
    "            live = Live(exp_name=config['model']['exp_name'], exp_message=config['model']['exp_message'])\n",
    "            # Train the current fold\n",
    "            Train_fold(train_index, val_index, fold_number, numpy_data, weight_dict, live)\n",
    "\n",
    "            fold_number += 1\n",
    "\n",
    "        print(\"Cross-validation training completed.\\n\")\n",
    "        Live.end()  # End the DVCLive run\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during cross-validation training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = load_df();\n",
    "    numpy_data = gather_numpy_files(DATA_PATH)\n",
    "\n",
    "    # Calculate weights\n",
    "    weight_dict = calculate_class_weights(df, 'downsampled_label')\n",
    "\n",
    "    # Create model\n",
    "    convolutional_model = create_model()\n",
    "    convolutional_model.summary()\n",
    "\n",
    "    # Train model\n",
    "    Cross_validation_training(numpy_data, weight_dict)\n",
    "\n",
    "\n",
    "    return numpy_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight dict for model: {0: 0.11450662739322537, 1: 0.8854933726067746}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 1)]           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 32, 32)            96        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32)           128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 16, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 16, 64)            6208      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 8, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 8, 128)            24704     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 128)           512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 4, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 426,145\n",
      "Trainable params: 425,697\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Training fold 1\n",
      "Training fold 1...\n",
      "x_val shape: (4988, 32, 1)\n",
      "y_val shape: (4988,)\n",
      "Starting training...\n",
      "Epoch 1/2\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8161 - binary_accuracy: 0.7748 - auc: 0.9291 - precision: 0.7044 - recall: 0.9519 - f1_score: 0.8097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:2319: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "293/293 [==============================] - 16s 28ms/step - loss: 0.8161 - binary_accuracy: 0.7748 - auc: 0.9291 - precision: 0.7044 - recall: 0.9519 - f1_score: 0.8097 - val_loss: 4.5351 - val_binary_accuracy: 0.1204 - val_auc: 0.6245 - val_precision: 0.1204 - val_recall: 1.0000 - val_f1_score: 0.2149 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4444 - binary_accuracy: 0.7585 - auc: 0.9352 - precision: 0.6830 - recall: 0.9685 - f1_score: 0.8010WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "293/293 [==============================] - 8s 29ms/step - loss: 0.4439 - binary_accuracy: 0.7590 - auc: 0.9353 - precision: 0.6840 - recall: 0.9686 - f1_score: 0.8018 - val_loss: 3.5440 - val_binary_accuracy: 0.1323 - val_auc: 0.7501 - val_precision: 0.1218 - val_recall: 1.0000 - val_f1_score: 0.2172 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dvclive:Error in save_dvc_exp: output '..\\dvclive\\plots\\metrics' does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/model_1.h5\n",
      "Training fold 1 completed\n",
      "\n",
      "Training fold 2\n",
      "Training fold 2...\n",
      "x_val shape: (4988, 32, 1)\n",
      "y_val shape: (4988,)\n",
      "Starting training...\n",
      "Epoch 1/2\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8145 - binary_accuracy: 0.7748 - auc: 0.9308 - precision: 0.7023 - recall: 0.9530 - f1_score: 0.8087WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "293/293 [==============================] - 11s 27ms/step - loss: 0.8145 - binary_accuracy: 0.7748 - auc: 0.9308 - precision: 0.7023 - recall: 0.9530 - f1_score: 0.8087 - val_loss: 4.5322 - val_binary_accuracy: 0.1204 - val_auc: 0.6206 - val_precision: 0.1204 - val_recall: 1.0000 - val_f1_score: 0.2149 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "291/293 [============================>.] - ETA: 0s - loss: 0.4462 - binary_accuracy: 0.7719 - auc: 0.9349 - precision: 0.6946 - recall: 0.9650 - f1_score: 0.8077WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "293/293 [==============================] - 8s 28ms/step - loss: 0.4451 - binary_accuracy: 0.7731 - auc: 0.9353 - precision: 0.6970 - recall: 0.9654 - f1_score: 0.8095 - val_loss: 3.8558 - val_binary_accuracy: 0.1438 - val_auc: 0.7353 - val_precision: 0.1233 - val_recall: 1.0000 - val_f1_score: 0.2195 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dvclive:Error in save_dvc_exp: output '..\\dvclive\\plots\\metrics' does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/model_2.h5\n",
      "Training fold 2 completed\n",
      "\n",
      "Training fold 3\n",
      "Training fold 3...\n",
      "x_val shape: (4988, 32, 1)\n",
      "y_val shape: (4988,)\n",
      "Starting training...\n",
      "Epoch 1/2\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7963 - binary_accuracy: 0.7721 - auc: 0.9285 - precision: 0.6995 - recall: 0.9525 - f1_score: 0.8067WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "293/293 [==============================] - 13s 31ms/step - loss: 0.7963 - binary_accuracy: 0.7721 - auc: 0.9285 - precision: 0.6995 - recall: 0.9525 - f1_score: 0.8067 - val_loss: 4.3966 - val_binary_accuracy: 0.1204 - val_auc: 0.6252 - val_precision: 0.1204 - val_recall: 1.0000 - val_f1_score: 0.2149 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.4208 - binary_accuracy: 0.7674 - auc: 0.9336 - precision: 0.6907 - recall: 0.9650 - f1_score: 0.8051WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "293/293 [==============================] - 9s 31ms/step - loss: 0.4203 - binary_accuracy: 0.7681 - auc: 0.9339 - precision: 0.6919 - recall: 0.9651 - f1_score: 0.8060 - val_loss: 4.2926 - val_binary_accuracy: 0.1452 - val_auc: 0.7099 - val_precision: 0.1231 - val_recall: 0.9965 - val_f1_score: 0.2192 - lr: 0.0010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m numpy \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m convolutional_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mCross_validation_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m numpy_data\n",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m, in \u001b[0;36mCross_validation_training\u001b[1;34m(numpy_data, weight_dict)\u001b[0m\n\u001b[0;32m     13\u001b[0m     live \u001b[38;5;241m=\u001b[39m Live(exp_name\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_name\u001b[39m\u001b[38;5;124m'\u001b[39m], exp_message\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_message\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Train the current fold\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mTrain_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumpy_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     fold_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-validation training completed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m, in \u001b[0;36mTrain_fold\u001b[1;34m(train_index, val_index, fold_number, numpy_data, weight_dict, live)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_dict\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Create models directory if it doesn't exist\u001b[39;00m\n\u001b[0;32m     35\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(MODEL_PATH, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1637\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler\n\u001b[1;32m-> 1637\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:536\u001b[0m, in \u001b[0;36mCallbackList.on_train_end\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m    534\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m--> 536\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\dvclive\\keras.py:28\u001b[0m, in \u001b[0;36mDVCLiveCallback.on_train_end\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, logs: Optional[Dict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\dvclive\\live.py:953\u001b[0m, in \u001b[0;36mLive.end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dvcyaml:\n\u001b[0;32m    949\u001b[0m         catch_and_warn(DvcException, logger)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dvc_repo\u001b[38;5;241m.\u001b[39mscm\u001b[38;5;241m.\u001b[39madd)(\n\u001b[0;32m    950\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdvc_file\n\u001b[0;32m    951\u001b[0m         )\n\u001b[1;32m--> 953\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_dvc_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_studio_updates_posted()\n\u001b[0;32m    957\u001b[0m \u001b[38;5;66;03m# Mark experiment as done\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\dvclive\\utils.py:182\u001b[0m, in \u001b[0;36mcatch_and_warn.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    184\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\dvclive\\live.py:985\u001b[0m, in \u001b[0;36mLive.save_dvc_exp\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dvcyaml:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_include_untracked\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdvc_file)\n\u001b[1;32m--> 985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_rev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dvc_repo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_untracked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_include_untracked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exp_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    990\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\dvc\\repo\\experiments\\__init__.py:359\u001b[0m, in \u001b[0;36mExperiments.save\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdvc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\dvc\\repo\\experiments\\save.py:32\u001b[0m, in \u001b[0;36msave\u001b[1;34m(repo, targets, name, recursive, force, include_untracked, message)\u001b[0m\n\u001b[0;32m     29\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving workspace in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[0;32m     31\u001b[0m queue \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mexperiments\u001b[38;5;241m.\u001b[39mworkspace_queue\n\u001b[1;32m---> 32\u001b[0m entry \u001b[38;5;241m=\u001b[39m \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m executor \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39minit_executor(repo\u001b[38;5;241m.\u001b[39mexperiments, entry)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\dvc\\repo\\experiments\\__init__.py:218\u001b[0m, in \u001b[0;36mExperiments.new\u001b[1;34m(self, queue, *args, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscm\u001b[38;5;241m.\u001b[39mget_ref(\u001b[38;5;28mstr\u001b[39m(exp_ref)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force:\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExperimentExistsError(exp_ref\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\dvc\\repo\\experiments\\queue\\workspace.py:38\u001b[0m, in \u001b[0;36mWorkspaceQueue.put\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy_paths\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_exp_rwlock(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo, writes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkspace\u001b[39m\u001b[38;5;124m\"\u001b[39m, WORKSPACE_STASH]):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stash_exp(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\dvc\\repo\\experiments\\queue\\base.py:310\u001b[0m, in \u001b[0;36mBaseStashQueue._stash_exp\u001b[1;34m(self, params, baseline_rev, branch, name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Stash changes from the workspace as an experiment.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    https://hydra.cc/docs/next/advanced/override_grammar/basic/\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscm\u001b[38;5;241m.\u001b[39mstash_workspace(reinstate_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m workspace:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscm\u001b[38;5;241m.\u001b[39mdetach_head(client\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdvc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m orig_head:\n\u001b[0;32m    311\u001b[0m         stash_head \u001b[38;5;241m=\u001b[39m orig_head\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m baseline_rev \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\scmrepo\\git\\__init__.py:449\u001b[0m, in \u001b[0;36mGit.detach_head\u001b[1;34m(self, rev, force, client)\u001b[0m\n\u001b[0;32m    447\u001b[0m orig_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_ref(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m, follow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    448\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetaching HEAD at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, rev)\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_ref(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\scmrepo\\git\\__init__.py:307\u001b[0m, in \u001b[0;36mGit._backend_func\u001b[1;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends[key]\n\u001b[0;32m    306\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(backend, name)\n\u001b[1;32m--> 307\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_backend \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmove_to_end(key, last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\scmrepo\\git\\backend\\pygit2\\__init__.py:364\u001b[0m, in \u001b[0;36mPygit2Backend.checkout\u001b[1;34m(self, branch, create_new, force, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, GitError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RevError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown Git revision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m detach \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetach\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ref \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m detach:\n",
      "File \u001b[1;32mc:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\pygit2\\repository.py:285\u001b[0m, in \u001b[0;36mBaseRepository.checkout_tree\u001b[1;34m(self, treeish, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m cptr \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgit_object **\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    284\u001b[0m ffi\u001b[38;5;241m.\u001b[39mbuffer(cptr)[:] \u001b[38;5;241m=\u001b[39m treeish\u001b[38;5;241m.\u001b[39m_pointer[:]\n\u001b[1;32m--> 285\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgit_checkout_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcptr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m payload\u001b[38;5;241m.\u001b[39mcheck_error(err)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "numpy = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

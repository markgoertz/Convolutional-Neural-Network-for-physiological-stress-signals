{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.callbacks\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.losses\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC, Precision, Recall, Metric\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model  # Import Model here\n",
    "\n",
    "\n",
    "import dvc.api\n",
    "from dvclive import Live\n",
    "from dvclive.keras import DVCLiveCallback  # Import the callback\n",
    "import yaml\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data/results\"\n",
    "MODEL_PATH = MAIN_PATH + \"/models\"\n",
    "LOG_PATH = MAIN_PATH + \"/logs\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "BEST_VAL_SCORE = 0\n",
    "BEST_MODEL = None\n",
    "HISTORY = []  # Initialize history_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_x_train(x_train):\n",
    "    # Initialize an empty dictionary for the restructured x_train\n",
    "    x_train_restructured = {}\n",
    "    # Loop through the first subject to get the signal keys automatically\n",
    "    first_subject_signals = list(x_train[list(x_train.keys())[0]].keys())\n",
    "    \n",
    "    # Initialize empty lists for each signal type based on the first subject's data\n",
    "    for signal in first_subject_signals:\n",
    "        x_train_restructured[signal] = []\n",
    "    # Loop through each subject's data and aggregate the signals\n",
    "    for subject, signals in x_train.items():\n",
    "        print(f\"Subject: {subject}\")\n",
    "        for signal, data in signals.items():\n",
    "            x_train_restructured[signal].append(data)\n",
    "    # Convert lists of windows into single numpy arrays for each signal type\n",
    "    for signal in x_train_restructured:\n",
    "        # Concatenate data for each signal type across subjects (e.g., all windows for BVP across subjects)\n",
    "        x_train_restructured[signal] = np.concatenate(x_train_restructured[signal], axis=0)\n",
    "    return x_train_restructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    return dvc.api.params_show(\"params.yaml\")\n",
    "\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_metrics(history_dict: dict):\n",
    "    total_plots = len(history_dict)\n",
    "    cols = total_plots // 2\n",
    "    rows = total_plots // cols\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "\n",
    "    pos = range(1, total_plots + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (key, value) in enumerate(history_dict.items()):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(range(len(value)), value)\n",
    "        plt.title(str(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df():\n",
    "    df = pd.read_csv(MAIN_PATH + \"/data/result_df.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_missing_values(numpy_data):\n",
    "    numpy_data['x_train'], numpy_data['y_train'] = Remove_missing_values(numpy_data['x_train'], numpy_data['y_train'])\n",
    "    numpy_data['x_val'], numpy_data['y_val'] = Remove_missing_values(numpy_data['x_val'], numpy_data['y_val'])\n",
    "    numpy_data['x_test_1'], numpy_data['y_test_1'] = Remove_missing_values(numpy_data['x_test_1'], numpy_data['y_test_1'])\n",
    "    numpy_data['x_test_2'], numpy_data['y_test_2'] = Remove_missing_values(numpy_data['x_test_2'], numpy_data['y_test_2'])\n",
    "    \n",
    "    return numpy_data\n",
    "\n",
    "def Remove_missing_values(x_data, y_data):\n",
    "    # Check if y_data contains missing values (NaNs) and remove corresponding x_data rows\n",
    "    valid_indices = ~np.isnan(y_data)  # Find valid (non-NaN) indices in y_data\n",
    "    x_clean = x_data[valid_indices]\n",
    "    y_clean = y_data[valid_indices]\n",
    "    return x_clean, y_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_pickle(directory):\n",
    "    \"\"\"\n",
    "    Load all data from pickle files in the specified directory.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Loaded training and test data as dictionaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pkl'):\n",
    "            key = filename.split('.')[0]  # Use the filename without extension as key\n",
    "            data[key] = pd.read_pickle(os.path.join(directory, filename))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(df, label_column):\n",
    "    vals_dict = {}\n",
    "    for i in df[label_column]:\n",
    "        if i in vals_dict.keys():\n",
    "            vals_dict[i] += 1\n",
    "        else:\n",
    "            vals_dict[i] = 1\n",
    "    total = sum(vals_dict.values())\n",
    "    weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n",
    "\n",
    "    print(f\"Weight dict for model: {weight_dict}\")\n",
    "    return weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class F1Score(Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_head(input_layer):\n",
    "    # First convolutional layer\n",
    "    x = tf.keras.layers.Conv1D(filters=32, kernel_size=config[\"model\"][\"kernel_size\"], \n",
    "                      activation=config[\"model\"][\"activation\"], padding=\"same\", \n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001))(input_layer)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    x = tf.keras.layers.Conv1D(64, kernel_size=config[\"model\"][\"kernel_size\"], activation=config[\"model\"][\"activation\"])(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)   \n",
    "    \n",
    "    x = tf.keras.layers.Conv1D(filters=128, kernel_size=config[\"model\"][\"kernel_size\"], \n",
    "                      activation=config[\"model\"][\"activation\"], padding=\"same\", \n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Flatten the output\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_layers, model_heads):\n",
    "    # Merge models using their outputs directly\n",
    "    combined = tf.keras.layers.concatenate(model_heads)\n",
    "\n",
    "    # Add additional layers after merging\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(combined)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # Adjust based on your task\n",
    "\n",
    "    # Final model\n",
    "    model = keras.Model(inputs=input_layers, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(input_layers, model_heads):\n",
    "    model = build_model(input_layers, model_heads)\n",
    "    optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=config[\"model\"][\"learning_rate\"])\n",
    "    loss = keras.losses.binary_crossentropy  # Ensure this is a callable, not a result\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            F1Score(name='f1_score')\n",
    "        ]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_val, y_val, class_weight):\n",
    "    \"\"\"Trains the model on the training data.\"\"\"\n",
    "\n",
    "    with Live() as live:\n",
    "        for epoch in range(config['model']['epochs']):\n",
    "            model.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                validation_data=(x_val, y_val),\n",
    "                epochs=1,  # Train for one epoch at a time\n",
    "                class_weight=class_weight,\n",
    "                callbacks=[\n",
    "                    keras.callbacks.ModelCheckpoint(\n",
    "                        filepath=os.path.join(MODEL_PATH, 'best_model.keras'),  # Add filepath argument\n",
    "                        save_best_only=True,\n",
    "                        monitor=\"val_binary_accuracy\"\n",
    "                    ),\n",
    "                    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "                    DVCLiveCallback(live=live)  # Add DVCLiveCallback to the list\n",
    "                ]\n",
    "            )\n",
    "            live.log_params(config)\n",
    "            live.log_artifact(\n",
    "                os.path.join(MODEL_PATH, 'best_model.h5'),\n",
    "                type=\"model\",\n",
    "                desc=\"This is a convolutional neural network model that is developed to detect stress.\",\n",
    "                labels=[\"no-stress\", \"stress\"],\n",
    "            )\n",
    "\n",
    "        model.save(os.path.join(MODEL_PATH, 'best_model.h5'))\n",
    "        live.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_to_json(history, fold_number, best_model):\n",
    "    # Create a dictionary for the current fold's metrics\n",
    "    metrics = {\n",
    "        \"fold_number\": fold_number,\n",
    "        \"val_accuracy\": history.history['val_accuracy'][-1],\n",
    "        \"val_loss\": history.history['val_loss'][-1],\n",
    "        \"best_model\": best_model\n",
    "    }\n",
    "\n",
    "    # Load existing metrics if the file exists\n",
    "    if os.path.exists('metrics.json'):\n",
    "        with open('metrics.json', 'r') as f:\n",
    "            existing_metrics = json.load(f)\n",
    "    else:\n",
    "        existing_metrics = []\n",
    "\n",
    "    # Append the new metrics\n",
    "    existing_metrics.append(metrics)\n",
    "\n",
    "    # Write back the updated metrics to the file\n",
    "    with open('metrics.json', 'w') as f:\n",
    "        json.dump(existing_metrics, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preparing_model(x_train, y_train, x_test, y_test, weight_dict):\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)  # Ensure the model path exists\n",
    "\n",
    "    try:\n",
    "        # Create the model heads\n",
    "        model_heads = []\n",
    "        input_layers = []\n",
    "        \n",
    "        for metric in config['model']['metrics']:\n",
    "            input_shape = (config['model']['input_shapes'][metric], config['model']['input_features'])\n",
    "            input_layer = tf.keras.layers.Input(shape=input_shape, name=f'input_{metric.lower()}')\n",
    "            input_layers.append(input_layer)\n",
    "            print(f\"Input shape for {metric}: {input_shape}\")\n",
    "            \n",
    "            # Create a model head for each input\n",
    "            model_head = create_model_head(input_layer)\n",
    "            model_heads.append(model_head)\n",
    "\n",
    "        print(f\"Model heads created: {model_heads}\")\n",
    "\n",
    "        model = compile_model(input_layers, model_heads)\n",
    "\n",
    "        train_model(\n",
    "            model,\n",
    "            [x_train[metric] for metric in config['model']['metrics']],\n",
    "            y_train,\n",
    "            [x_test[metric] for metric in config['model']['metrics']],\n",
    "            y_test,\n",
    "            weight_dict\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during preparing: {type(e).__name__}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(data, metrics):\n",
    "    filtered_data = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            filtered_data[key] = {k: v for k, v in value.items() if k in metrics}\n",
    "        else:\n",
    "            filtered_data[key] = value\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def main():\n",
    "    df = load_df()\n",
    "    datasets = load_data_from_pickle(DATA_PATH)\n",
    " \n",
    "    # Extract the datasets\n",
    "    x_train = datasets['x_train']\n",
    "    x_train_restructured = restructure_x_train(x_train)\n",
    "    y_train = datasets['y_train']\n",
    "    x_val = datasets['x_val']\n",
    "    x_val_restructured = restructure_x_train(x_val)\n",
    "    datasets['x_train'] = x_train_restructured\n",
    "    datasets['x_val'] = x_val_restructured\n",
    "\n",
    "    y_val = datasets['y_val']\n",
    "\n",
    "\n",
    "    # Calculate weights\n",
    "    weight_dict = calculate_class_weights(df, 'downsampled_label')\n",
    "\n",
    "    # Filter columns based on config['model']['metrics']\n",
    "    datasets = filter_columns(datasets, config['model']['metrics'])\n",
    "\n",
    "    for key, value in datasets.items():\n",
    "        if isinstance(value, dict):\n",
    "            for sub_key, sub_value in value.items():\n",
    "                print(f\"Shape of {key}_{sub_key}: {sub_value.shape}\")\n",
    "        else:\n",
    "            print(f\"Shape of {key}: {value.shape}\")\n",
    "\n",
    "    # Train model\n",
    "    # x = Preparing_model(x_train, y_train, x_val, y_val, weight_dict)\n",
    "    print(f\"Model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['S10', 'S11', 'S13', 'S14', 'S15', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9'])\n",
      "Weight dict for model: {0: 0.11450662739322537, 1: 0.8854933726067746}\n",
      "Shape of x_test_1_EDA: (703, 32, 1)\n",
      "Shape of x_test_2_EDA: (740, 32, 1)\n",
      "Shape of y_test_1: (703,)\n",
      "Shape of y_test_2: (740,)\n",
      "Shape of y_train: (11610, 1)\n",
      "Model training completed\n"
     ]
    }
   ],
   "source": [
    "x = main()\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC, Precision, Recall, Metric\n",
    "\n",
    "from dvclive import Live  # Ensure DVCLive is imported\n",
    "from dvclive.keras import DVCLiveCallback  # Import the callback\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = MAIN_PATH + \"/data/numpy\"\n",
    "MODEL_PATH = MAIN_PATH + \"/models\"\n",
    "LOG_PATH = MAIN_PATH + \"/logs\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "BEST_VAL_SCORE = 0\n",
    "BEST_MODEL = None\n",
    "HISTORY = []  # Initialize history_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path=MAIN_PATH + \"/params.yaml\"):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = load_config()  # This loads the configuration once and allows direct access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_metrics(history_dict: dict):\n",
    "    total_plots = len(history_dict)\n",
    "    cols = total_plots // 2\n",
    "    rows = total_plots // cols\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "\n",
    "    pos = range(1, total_plots + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (key, value) in enumerate(history_dict.items()):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(range(len(value)), value)\n",
    "        plt.title(str(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df():\n",
    "    df = pd.read_csv(MAIN_PATH + \"/data/result_df.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_missing_values(numpy_data):\n",
    "    numpy_data['x_train'], numpy_data['y_train'] = Remove_missing_values(numpy_data['x_train'], numpy_data['y_train'])\n",
    "    numpy_data['x_val'], numpy_data['y_val'] = Remove_missing_values(numpy_data['x_val'], numpy_data['y_val'])\n",
    "    numpy_data['x_test_1'], numpy_data['y_test_1'] = Remove_missing_values(numpy_data['x_test_1'], numpy_data['y_test_1'])\n",
    "    numpy_data['x_test_2'], numpy_data['y_test_2'] = Remove_missing_values(numpy_data['x_test_2'], numpy_data['y_test_2'])\n",
    "    \n",
    "    return numpy_data\n",
    "\n",
    "def Remove_missing_values(x_data, y_data):\n",
    "    # Check if y_data contains missing values (NaNs) and remove corresponding x_data rows\n",
    "    valid_indices = ~np.isnan(y_data)  # Find valid (non-NaN) indices in y_data\n",
    "    x_clean = x_data[valid_indices]\n",
    "    y_clean = y_data[valid_indices]\n",
    "    return x_clean, y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_numpy_files(data_path):\n",
    "    numpy_data = {}\n",
    "    \n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith('.npy'):\n",
    "            file_path = os.path.join(data_path, file_name)\n",
    "            numpy_data[file_name[:-4]] = np.load(file_path)  # Store in dict\n",
    "    \n",
    "    # Clean data by removing rows where y_* contains missing values\n",
    "    numpy_data = Clean_missing_values(numpy_data)\n",
    "\n",
    "    return numpy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(df, label_column):\n",
    "    vals_dict = {}\n",
    "    for i in df[label_column]:\n",
    "        if i in vals_dict.keys():\n",
    "            vals_dict[i] += 1\n",
    "        else:\n",
    "            vals_dict[i] = 1\n",
    "    total = sum(vals_dict.values())\n",
    "    weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n",
    "\n",
    "    print(f\"Weight dict for model: {weight_dict}\")\n",
    "    return weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_layer = keras.Input(shape=(config[\"model\"][\"input_shape\"], config[\"model\"][\"input_features\"]))\n",
    "    \n",
    "    x = layers.Conv1D(filters=32, kernel_size=config[\"model\"][\"kernel_size\"], activation=config[\"model\"][\"activation\"], padding=\"same\", kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, activation=config[\"model\"][\"activation\"], padding=\"same\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters=128, kernel_size=3, activation=config[\"model\"][\"activation\"], padding=\"same\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compile_model():\n",
    "    model = create_model()\n",
    "    optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=config[\"model\"][\"learning_rate\"])\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            F1Score(name='f1_score')\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDatasetForFolds(train_index, validation_index, fold_nr, numpy_data):\n",
    "    print(f\"Training fold {fold_nr}...\")\n",
    "\n",
    "    # Split the data into train sets for this fold.\n",
    "    x_train_fold = numpy_data['x_train'][train_index]\n",
    "    y_train_fold = numpy_data['y_train'][train_index]\n",
    "\n",
    "    print(f\"x_val shape: {numpy_data['x_val'].shape}\")\n",
    "    print(f\"y_val shape: {numpy_data['y_val'].shape}\")\n",
    "    \n",
    "    # Ensure to use only the training set indices\n",
    "    x_validation_fold = numpy_data['x_val'][:len(validation_index)]  # Taking the first `len(validation_index)` samples\n",
    "    y_validation_fold = numpy_data['y_val'][:len(validation_index)]\n",
    "\n",
    "    # Create tf.data.Datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_fold, y_train_fold))\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((x_validation_fold, y_validation_fold))\n",
    "    test_dataset_subject1 = tf.data.Dataset.from_tensor_slices((numpy_data['x_test_1'], numpy_data['y_test_1']))\t\n",
    "    test_dataset_subject2 = tf.data.Dataset.from_tensor_slices((numpy_data['x_test_2'], numpy_data['y_test_2']))\n",
    "    \n",
    "    # Shuffling and batching the datasets\n",
    "    train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    validation_dataset = validation_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    test_dataset_subject1 = test_dataset_subject1.batch(BATCH_SIZE)\n",
    "    test_dataset_subject2 = test_dataset_subject2.batch(BATCH_SIZE)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset_subject1, test_dataset_subject2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_native(data):\n",
    "    \"\"\"Recursively convert numpy types to native python types.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_to_native(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_to_native(item) for item in data]\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return data.tolist()  # Convert numpy array to list\n",
    "    elif isinstance(data, (np.float32, np.float64)):\n",
    "        return data.item()  # Convert single value numpy float to Python float\n",
    "    else:\n",
    "        return data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_to_json(history, fold_number, best_model):\n",
    "    # Create a dictionary for the current fold's metrics\n",
    "    metrics = {\n",
    "        \"fold_number\": fold_number,\n",
    "        \"val_accuracy\": history.history['val_accuracy'][-1],\n",
    "        \"val_loss\": history.history['val_loss'][-1],\n",
    "        \"best_model\": best_model\n",
    "    }\n",
    "\n",
    "    # Load existing metrics if the file exists\n",
    "    if os.path.exists('metrics.json'):\n",
    "        with open('metrics.json', 'r') as f:\n",
    "            existing_metrics = json.load(f)\n",
    "    else:\n",
    "        existing_metrics = []\n",
    "\n",
    "    # Append the new metrics\n",
    "    existing_metrics.append(metrics)\n",
    "\n",
    "    # Write back the updated metrics to the file\n",
    "    with open('metrics.json', 'w') as f:\n",
    "        json.dump(existing_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_fold(train_index, val_index, fold_number, numpy_data, weight_dict, live):\n",
    "    global BEST_VAL_SCORE, HISTORY \n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    train_dataset, validation_dataset, test_sj1, test_sj2 = SplitDatasetForFolds(train_index, val_index, fold_number, numpy_data)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Compile_model()\n",
    "\n",
    "    # Set up DVC Live callback for this fold\n",
    "    live_callback = DVCLiveCallback()  # No need to pass live here\n",
    "\n",
    "    # Set up other callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(MODEL_PATH, f\"best_model_fold_{fold_number}.keras\"),\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_accuracy\"\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "        live_callback  # Add DVCLiveCallback to the list\n",
    "    ]\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=config['model']['epochs'],\n",
    "        validation_data=(validation_dataset),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=weight_dict\n",
    "    )\n",
    "\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "    \n",
    "    # Save the model to the models directory\n",
    "    model.save(os.path.join(MODEL_PATH, f'model_{fold_number}.h5'))\n",
    "    print(f'Model saved to models/model_{fold_number}.h5')\n",
    "\n",
    "    print(f\"Training fold {fold_number} completed\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cross_validation_training(numpy_data, weight_dict):\n",
    "    scores = []\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)  # Ensure the model path exists\n",
    "\n",
    "    # Initialize KFold with the number of splits\n",
    "    kfold = KFold(n_splits=config['model']['folds'], shuffle=True, random_state=42)\n",
    "\n",
    "    try:\n",
    "        for fold_number, (train_index, val_index) in enumerate(kfold.split(numpy_data['x_train']), start=1):\n",
    "            print(f\"Training fold {fold_number}\")\n",
    "            \n",
    "            # Modify the experiment name to a simpler format without \"branch\" or underscores\n",
    "            exp_name = f\"fold-{fold_number}\".lower()\n",
    "\n",
    "            # Create a Live instance with the updated experiment name\n",
    "            live = Live(exp_name=exp_name,\n",
    "                        exp_message=f\"Training fold {fold_number}\")\n",
    "\n",
    "            # Train the current fold and calculate score\n",
    "            score = Train_fold(train_index, val_index, fold_number, numpy_data, weight_dict, live)\n",
    "            scores.append(score)\n",
    "\n",
    "            live.end()  # End the live logging for this fold\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during cross-validation training: {e}\")\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = load_df();\n",
    "    numpy_data = gather_numpy_files(DATA_PATH)\n",
    "\n",
    "    # Calculate weights\n",
    "    weight_dict = calculate_class_weights(df, 'downsampled_label')\n",
    "\n",
    "    # Create model\n",
    "    convolutional_model = create_model()\n",
    "    convolutional_model.summary()\n",
    "\n",
    "    # Train model\n",
    "    Cross_validation_training(numpy_data, weight_dict)\n",
    "\n",
    "\n",
    "    return numpy_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight dict for model: {0: 0.11450662739322537, 1: 0.8854933726067746}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 1)]           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 32, 32)            128       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32)           128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 16, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 16, 64)            6208      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 8, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 8, 128)            24704     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 128)           512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 4, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 426,177\n",
      "Trainable params: 425,729\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Training fold 1\n",
      "Training fold 1...\n",
      "x_val shape: (4988, 32, 1)\n",
      "y_val shape: (4988,)\n",
      "Starting training...\n",
      "Epoch 1/3\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.8585 - binary_accuracy: 0.7697 - auc: 0.9221 - precision: 0.6990 - recall: 0.9481 - f1_score: 0.8047"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\goert\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:2319: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "244/244 [==============================] - 16s 35ms/step - loss: 0.8572 - binary_accuracy: 0.7705 - auc: 0.9225 - precision: 0.7006 - recall: 0.9485 - f1_score: 0.8059 - val_loss: 3.6429 - val_binary_accuracy: 0.3612 - val_auc: 0.5484 - val_precision: 0.3612 - val_recall: 1.0000 - val_f1_score: 0.5307 - lr: 0.0010\n",
      "Epoch 2/3\n",
      "242/244 [============================>.] - ETA: 0s - loss: 0.5064 - binary_accuracy: 0.7709 - auc: 0.9293 - precision: 0.6948 - recall: 0.9640 - f1_score: 0.8076WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "244/244 [==============================] - 8s 34ms/step - loss: 0.5049 - binary_accuracy: 0.7724 - auc: 0.9299 - precision: 0.6979 - recall: 0.9646 - f1_score: 0.8098 - val_loss: 2.3251 - val_binary_accuracy: 0.3645 - val_auc: 0.7744 - val_precision: 0.3624 - val_recall: 1.0000 - val_f1_score: 0.5320 - lr: 0.0010\n",
      "Epoch 3/3\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.3200 - binary_accuracy: 0.7746 - auc: 0.9341 - precision: 0.6983 - recall: 0.9707 - f1_score: 0.8123WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "244/244 [==============================] - 8s 33ms/step - loss: 0.3200 - binary_accuracy: 0.7746 - auc: 0.9341 - precision: 0.6983 - recall: 0.9707 - f1_score: 0.8123 - val_loss: 2.9535 - val_binary_accuracy: 0.3919 - val_auc: 0.7558 - val_precision: 0.3726 - val_recall: 0.9993 - val_f1_score: 0.5428 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to validate remotes. Disabling auto push: config file error: no remote specified in d:\\Master of Applied IT. Create a default remote with\n",
      "    dvc remote add -d <remote name> <remote url>\n",
      "WARNING: The following untracked files were present in the workspace before saving but will not be included in the experiment commit:\n",
      "\tconfig.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/model_1.h5\n",
      "Training fold 1 completed\n",
      "\n",
      "Fold 1 metrics logged.\n",
      "Training fold 2\n",
      "Training fold 2...\n",
      "x_val shape: (4988, 32, 1)\n",
      "y_val shape: (4988,)\n",
      "Starting training...\n",
      "Epoch 1/3\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.8706 - binary_accuracy: 0.7675 - auc: 0.9158 - precision: 0.6945 - recall: 0.9533 - f1_score: 0.8036WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "244/244 [==============================] - 12s 34ms/step - loss: 0.8706 - binary_accuracy: 0.7675 - auc: 0.9158 - precision: 0.6945 - recall: 0.9533 - f1_score: 0.8036 - val_loss: 4.0194 - val_binary_accuracy: 0.3612 - val_auc: 0.5361 - val_precision: 0.3612 - val_recall: 1.0000 - val_f1_score: 0.5307 - lr: 0.0010\n",
      "Epoch 2/3\n",
      "242/244 [============================>.] - ETA: 0s - loss: 0.5371 - binary_accuracy: 0.7592 - auc: 0.9241 - precision: 0.6807 - recall: 0.9669 - f1_score: 0.7990WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "244/244 [==============================] - 8s 33ms/step - loss: 0.5354 - binary_accuracy: 0.7608 - auc: 0.9249 - precision: 0.6839 - recall: 0.9674 - f1_score: 0.8013 - val_loss: 3.7211 - val_binary_accuracy: 0.3612 - val_auc: 0.6796 - val_precision: 0.3612 - val_recall: 1.0000 - val_f1_score: 0.5307 - lr: 0.0010\n",
      "Epoch 3/3\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.3463 - binary_accuracy: 0.7560 - auc: 0.9281 - precision: 0.6778 - recall: 0.9700 - f1_score: 0.7980WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "244/244 [==============================] - 8s 35ms/step - loss: 0.3457 - binary_accuracy: 0.7570 - auc: 0.9285 - precision: 0.6796 - recall: 0.9702 - f1_score: 0.7993 - val_loss: 3.6416 - val_binary_accuracy: 0.3801 - val_auc: 0.6999 - val_precision: 0.3681 - val_recall: 1.0000 - val_f1_score: 0.5382 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "numpy = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
